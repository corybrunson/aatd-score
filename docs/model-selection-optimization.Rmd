---
title: "Selection and Optimization of Predictive Models of AAT Genotype"
author: "Jason Cory Brunson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      fig.width = 8)
library(tidyverse)
library(tidymodels)
theme_set(theme_bw())
sample_denom <- "6"
```

This phase of the project aims to complete two goals:

1. Determine the most effective model family to predict AAT genotype from lung and liver medical history.
2. Identify parameter settings for which this model family achieves the greatest predictive accuracy.

The goal of the next phase will be to define a clinically practical risk score for abnormal AAT genotype based on the optimized model.

# Phase 1: Model selection

We considered several families of predictive model: logistic regression (LR), linear discriminant analysis (LDA), decision tree (DT), random forest (RF), nearest neighbors (NN), and support vector machine (SVM) using 1-, 2-, and 3-degree polynomial specifications.
We will compare these models over a $3 \times 3$ grid of prediction tasks given by each combination of predictors (lung/liver medical history only, together with gender, and together with smoking history) and of response (ZZ, SZ/ZZ, or any non-MM).

```{r model selection results}
# read in model summary data
read_rds(here::here(str_c("data/aatd-1-copd-", sample_denom, ".rds"))) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_copd_res
read_rds(here::here(str_c("data/aatd-1-count-", sample_denom, ".rds"))) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_count
read_rds(here::here(str_c("data/aatd-1-metric-", sample_denom, ".rds"))) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_metric
read_rds(here::here(str_c("data/aatd-1-pred-", sample_denom, ".rds"))) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_pred
# model family groups
read_rds(here::here(str_c("data/aatd-1-pred-", sample_denom, ".rds"))) %>%
  distinct(model) %>%
  mutate(type = case_when(
    model == "logistic regression" | model == "linear discriminant" ~
      "generalized linear",
    model == "decision tree" | model == "random forest" ~ "tree-based",
    model == "nearest neighbor" ~ "distance-based",
    str_detect(model, "svm") ~ "kernel-based"
  )) %>%
  mutate(type == fct_inorder(type)) ->
  model_types
```

For this phase, models were evaluated using a fixed sample of $\frac{1}{`r sample_denom`}$ of the data.
The sample was stratified by a 4-way categorization of genotypes: ZZ, SZ, other non-MM, and MM. This was done in order to prevent significant changes in the balance of response values in each model.
Models were fit on a \frac{2}{3} training set of this sample and evaluated on the remaining \frac{1}{3} testing set. This partition was also stratified by genotype and fixed across all evaluations.

## Performance curves

These are all binary classification problems, such that our probabilistic models can be used to define a sliding scale of screening thresholds based on different prescriptive trade-offs between the false negative and false positive rates.
As such, we primarily use performance curves to compare them.
The response variables are highly imbalanced, due to the rarity of abnormal genotypes.
We therefore take advantage of two popular performance curves: The receiver operator characteristic (ROC) and precision--recall (PR) curves.
These have complementary strengths and may reveal different trade-offs between the data (Davis & Goadrich, 2006).

These models are being designed as candidate substitutes for the current recommendation that patients be screened for an abnormal alpha-1 genotype if (and only if) they have been diagnosed with chronic obstructive pulmonary disease (COPD).
In addition to the models' performance curves, we also include a marker at the sensitivity and specificity of this recommendation for each combination of predictors response.
(The recommendation is not directly affected by the choice of predictors, but the data may differ slightly as the use of a new predictor forces us to discard cases for which its value is missing, which will slightly change the _measured_ sensitivity and specificity of the recommendation.)

### Receiver operator curves

```{r roc plots, fig.height=8}
aatd_mod_res_pred %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_point(data = aatd_copd_res) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "bottom")
```

The ROC curves show LR and LDA to perform very similarly. This is to be expected, since LDA can be understood as a generalization of LR to more than two classes (though they are not equivalent in the two-class case).
NN models perform much less well, though still consistently better than DT and RF models, which are not visibly different from random screening.
In some cases, SVM models outperform DT and RF and are competitive with LR and LDA, but in many others they perform worse than random screening.

The ROC curves suggest that ZZ and, less so, SZ/ZZ genotypes are significantly easier to predict than arbitrary abnormal genotypes, and they demonstrate a modest benefit to including smoking history in most models but an uncertain benefit to including gender.
As evaluated on our data, the standard recommendation underperforms random screening for SZ/ZZ and for arbitrary abnormal genotype, though it does provide a slight benefit in screening for ZZ.

It is generally understood that sensitivity is more valuable than specificity in this context: The potential cost of missing an abnormal AAT genotype is much higher than that of an unnecessary screen.
Our primary focus, then, is the top half-space of each ROC plot.
Troublingly, COPD-based screening shows lower sensitivity than random screening for all three abnormal genotype sets, though only slightly in the case of ZZ.
The NN and RF models show little difference from random screening toward the top-left corner of each plot, and the LR and LDA models show much clearer improvement with the addition of gender or smoking history as predictors and only minor (< 5%) improvement without.
The SVM models remain poor and inconsistent predictors when restricted to the high-sensitivity region.

The table and plot below summarize the sensitivity gains of all model families relative to random screening and to COPD-based screening, respectively.

```{r sensitivity gain, fig.height=6}
# improvement relative to random screening
aatd_mod_res_pred %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  mutate(sensitivity_gain = sensitivity - 1 + specificity) %>%
  left_join(model_types, by = "model") %>%
  filter(response == "ZZ") %>%
  group_by(model, predictors) %>%
  filter(sensitivity_gain == max(sensitivity_gain)) %>%
  group_by(model) %>%
  filter(sensitivity_gain == min(sensitivity_gain) |
           sensitivity_gain == max(sensitivity_gain)) %>%
  mutate(bound = ifelse(
    sensitivity_gain == min(sensitivity_gain),
    "Minimum", "Maximum"
  )) %>%
  group_by(model, sensitivity_gain) %>%
  filter(.threshold == max(.threshold)) %>%
  slice_head(n = 1L) %>%
  ungroup() %>%
  mutate(bound = factor(bound, levels = c("Minimum", "Maximum"))) %>%
  select(
    Model = model,
    bound, `sensitivity gain` = sensitivity_gain
  ) %>%
  pivot_wider(names_from = bound, names_glue = "{bound} {.value}",
              values_from = `sensitivity gain`, names_sort = TRUE) %>%
  mutate(across(
    where(is.double),
    ~ str_c(format(round(., digits = 3L) * 100, nsmall = 1L, trim = TRUE), "%")
  )) %>%
  set_names(c(
    "Model",
    "Least peak sensitivity gain (ZZ)", "Greatest peak sensitivity gain (ZZ)"
  )) %>%
  knitr::kable(align = "lrr")
# improvement relative to COPD-based screening
aatd_mod_res_pred %>%
  left_join(model_types, by = "model") %>%
  filter(type == "generalized linear" & response == "ZZ") %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  rename_with(~ str_c("model_", .), c(sensitivity, specificity)) %>%
  left_join(aatd_copd_res, by = c("predictors", "response")) %>%
  rename_with(~ str_c("copd_", .), c(sensitivity, specificity)) %>%
  # sensitivity gain relative to COPD at minimum higher specificity
  mutate(
    model_copd_dist = abs(model_specificity - copd_specificity),
    model_copd_sign = sign(model_specificity - copd_specificity)
  ) %>%
  group_by(model, predictors, response, model_copd_sign) %>%
  filter(model_copd_dist == min(model_copd_dist)) %>%
  ungroup() %>%
  transmute(
    model, predictors, response, .threshold,
    specificity_diff = model_specificity - copd_specificity,
    sensitivity_diff = model_sensitivity - copd_sensitivity
  ) %>%
  unite("specification", predictors, response, sep = " -> ") %>%
  ggplot(aes(x = specificity_diff, y = sensitivity_diff,
             shape = model, color = specification)) +
  coord_equal() +
  geom_point() +
  geom_hline(yintercept = 0) + geom_vline(xintercept = 0) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  labs(x = "Change in specificity", y = "Change in sensitivity") +
  theme(legend.position = "right")
```

The greatest improvements in ZZ prediction over random screening are limited to the LR and LDA models and range from 17 to 20 percentage points.
These obtain near 50% specificity, which makes for useful comparisons against COPD-based screening.
For ZZ prediction (see the plot above), the LR and LDA models obtain between 15 and 21 percentage point improvement over COPD-based screening in sensitivity subject to a specificity loss of less than 5 percentage points.

### Precision--recall curves

Whereas recall is equivalent to sensitivity (true positive rate, the rate at which true cases test positive), precision is not the true negative rate (specificity) but the rate at which positive tests yield true cases (the positive predictive value, and the complement of the false discovery rate: $\text{PPV} = 1 - \text{FDR}$).
Precision--recall curves therefore map the cost, in decreased value of tests (upward along the ordinate), of increasing the detection of cases (rightward along the abscissa).
In the plots below, note that both recall and precision lie along log-transformed axes.

```{r pr plots, fig.height=6}
aatd_mod_res_pred %>%
  group_by(model, predictors, response) %>%
  pr_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = recall, y = precision)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  lims(x = c(0, 1), y = c(0, 1)) +
  # scale_x_continuous(trans = "log", breaks = breaks_log(base = 10),
  #                    labels = scales::label_percent()) +
  # scale_y_continuous(trans = "log", breaks = breaks_log(base = 10),
  #                    labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "bottom")
```

The model families are not as clearly distinguishable in terms of precision and recall.
The superiority of LR and LDA is discernible as the curves surpass 1% recall: the NN curves fall consistently below, and the DT and RF curves only appear to lie above them due to linear interpolation between measured values, which is inappropriate both for PR curves generally and for log--log plots.
When predicting ZZ, all curves attain peak precision between 1% and 10% recall and decline in precision as recall approaches 100%. (The pattern is similar for predicting SZ/ZZ.)
These differences are marginal, however---as can be seen when the PR curves are plotted on untransformed axes, all of these models perform quite poorly.

Taken together, the ROC and PR curves favor logistic regression (and linear discriminant analysis) as candidate bases for a new risk score.

# Phase 2: Model optimization

```{r model optimization results}
# read in evaluation data
read_rds(here::here("data/aatd-eval.rds")) %>%
  mutate(across(c(model, predictors, response), fct_inorder)) %>%
  mutate(.metric = fct_inorder(.metric)) %>%
  mutate(.metric = fct_recode(
    .metric,
    kappa = "kap", `mean log loss` = "mn_log_loss", AUROC = "roc_auc"
  )) ->
  aatd_metrics
```

We considered two model families for the optimization phase: logistic regression and random forest.
We omitted linear discriminant analysis from this phase because it closely matched the performance of logistic regression, and because it is less well-understood by clinical researchers and not commonly used to derive risk scores.
We omitted decision trees because they showed no benefit in any of the experiments in Phase 1.

We omitted nearest neighbors and support vector machines because they proved extremely time- and memory-intensive to fit. In the case of NN, this was due to limitations of the implementation: Our data comprise binary (and, in some cases, categorical) variables, so that the space of possible records is a hypercube. As a result, only a small number of neighborhoods can be defined around each case, using the Hamming distance defined by the number of variables on which two cases differ, and records are duplicated at high rates. In contrast, most NN implementations rely on the computation of all pairwise distances between cases, which is highly redundant in this setting. Without a weighting scheme on the variables, the NN approach is mismatched, and would require a new implementation to be computationally feasible.

In general, SVMs are less interpretable than LR, decision tree, and even RF and NN models, and thereby less suitable for risk score derivation, but they can be justified when their predictive performance significantly exceeds these approaches. Our preliminary experiments showed no evidence of superior performance---indeed, SVMs performed slightly worse than most other model families---so we omit them from consideration.

## Comparison of model families

Across the same sets of predictors and responses as in Phase 1, we evaluated LR and RF models using 6-fold cross validation on the entire data set:
For each 5--1 split of a 6-fold partition, models were _trained_ and _tuned_ on the $\frac{5}{6}$ part by identifying the parameter settings that obtained the best fit, then _tested_ on the remaining $\frac{1}{6}$.
Logistic regression models were tuned over 6 values of the penalty parameter ($10 ^ {-10}$ through $10 ^ 0$ with exponents at increments of $2$), which controls the cost of each additional predictor in the pruned model.
Random forest models were tuned over 10 parameter settings: each combination of 5 numbers of trees ($1$ through $100$ at rounded powers of $\sqrt{10}$) and 2 numbers of predictors per tree ($p ^ {1/3} \approx 3$ and $\sqrt{p} \approx 4$, where $p$ is the total number of predictors).
We focused on two performance measures: overall accuracy and AUROC.

By far the strongest determinant of predictive performance was the genotype subclass being predicted: prediction of ZZ alone achieved greatest accuracy (near 99%), prediction of SZ or ZZ was nearly as accurate, and prediction of any abnormal (non-MM) genotype was much more difficult (near 85%).
This pattern was much noisier, though still detectable, when evaluating performance by AUROC---though by this metric the model family was most determinative (logistic regression outperformed random forest on every task.)
This may be due not to the task of predicting ZZ genotypes being easier but to the population of ZZ cases being smaller, which can increase the nominal performance of a model by increasing the lower bounds on its sensitivity ... (cite paper)
Though, since ZZ cases are also the most clinically urgent to detect, we focus on this task when evaluating the optimized models.

### Range of performance across parameter settings

The plot below locates the accuracy and AUROC of each fitted ZZ prediction model over a range of parameter settings for each model family.
The results are separated by model family, model specification (predictors used), and evaluation measure.
The logistic regression models varied the penalty parameter while the random forest models varied the number of predictors used in each tree and the number of trees in the forest.

```{r performance across parameters}
# compare performance of ZZ models across parameter settings
aatd_metrics %>%
  filter(response == "ZZ") %>%
  unnest(hyperparameters) %>%
  group_by_at(vars(-id, -.estimate)) %>%
  summarize(mean = mean(.estimate)) %>%
  filter(.metric == "accuracy" | .metric == "AUROC") %>%
  mutate(predictors = fct_rev(predictors)) %>%
  ggplot(aes(x = mean, y = predictors)) +
  facet_grid(model ~ .metric, scales = "free_x") +
  geom_jitter(width = 0, height = 1/4, alpha = .5) +
  labs(x = "Performance", y = "Predictors") +
  ggtitle("Accuracy of predictive models of ZZ genotype")
```

Measured by overall accuracy, LR models showed more consistency across parameter settings, but some RF models achieved greater ultimate performance.
On the more important measure of AUROC, however, LR models universally outperformed RF models.
Among the LR models, the inclusion of gender did not improve performance by either measure, but the inclusion of smoking history led to consistent, though not highly significant, improvement (a less than 0.01% increase in accuracy and a less than 5% increase in AUROC).

### Optimized parameter settings

```{r optimal parameters}
# best parameter settings for each model and formula by two measures
aatd_metrics %>%
  filter(response == "ZZ") %>%
  filter(.metric == "accuracy" | .metric == "AUROC") %>%
  group_by_at(vars(-id, -.estimate)) %>%
  summarize(mean = mean(.estimate), sd = sd(.estimate), .groups = "drop") %>%
  group_by(model, predictors, response, .metric) %>%
  filter(mean == max(mean)) %>%
  unnest(hyperparameters) %>%
  mutate(param_pref = rank(ifelse(is.na(penalty), desc(trees), penalty))) %>%
  filter(param_pref == min(param_pref)) %>%
  ungroup() ->
  aatd_opt
```

Below we inspect the settings in each model that achieved the greatest performance, by overall accuracy and by the area under the receiver operator characteristic curve (AUROC):

```{r optimal performance table}
aatd_opt %>%
  mutate(
    penalty = ifelse(
      is.na(penalty), NA_character_,
      str_replace(format(penalty, scientific = TRUE), "e", " × 10^")
    ),
    mtry = as.character(mtry), trees = as.character(trees)
  ) %>%
  mutate(across(c(penalty, mtry, trees), ~ ifelse(is.na(.), "&ndash;", .))) %>%
  mutate(mean = format(round(mean, digits = 3L), digits = 3L, nsmall = 1L)) %>%
  select(
    Measure = .metric, Model = model, Predictors = predictors,
    penalty, mtry, trees,
    `Mean performance` = mean
  ) %>%
  arrange(Measure, Model, Predictors) %>%
  knitr::kable(align = "lllrrrr")
```

The high accuracy rates are largely artifacts of the imbalance in the data; the ROC curves provide a more discriminating measure of performance. These are used below to visually compare performance between the model families and specifications.
Remember that performance in each setting is an average taken over 6-fold cross-validation, from which we derive compatibility intervals.

```{r optimal performance plot, fig.height=4}
aatd_opt %>%
  filter(.metric == "AUROC") %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_rev(formula)) %>%
  ggplot(aes(x = mean, xmin = mean - 2*sd, xmax = mean + 2*sd, y = formula)) +
  facet_grid(model ~ .) +
  geom_pointrange() +
  labs(x = "AUROC", y = NULL)
```

Logistic regression clearly provides greater performance in this setting.

## Selection of optimized model

A remaining question is whether the inclusion of smoking history provides detectable improvement in the model.
We could perform a likelihood ratio test of the null hypothesis that the model with smoking history outperforms the model without, or use the Akaike Information Criterion to quantify the trade-off between predictive performance and model complexity.
