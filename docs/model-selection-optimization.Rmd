---
title: "Selection and Optimization of Predictive Models of AAT Genotype"
author: "Jason Cory Brunson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      fig.width = 8)
library(tidyverse)
library(tidymodels)
theme_set(theme_bw())
```

This phase of the project aims to complete two goals:

1. Determine the most effective model family to predict AAT genotype from lung and liver medical history.
2. Identify parameter settings for which this model family achieves the greatest predictive accuracy.

The goal of the next phase will be to define a clinically practical risk score for abnormal AAT genotype based on the optimized model.

# Phase 1: Model selection

We considered several families of predictive model: logistic regression (LR), linear discriminant analysis (LDA), decision rules (DR), decision tree (DT), random forest (RF), nearest neighbors (NN), and support vector machine (SVM) using 1-, 2-, and 3-degree polynomial specifications.
We also considered integer risk scores obtained using FasterRisk (FR), which would not need to be transformed in order to be clinically practical (Liu &al, 2022).
We will compare these models over a $3 \times 3$ grid of prediction tasks given by each combination of predictors (lung/liver medical history only, together with gender, and together with either smoking exposure or smoking history) and of response (ZZ, SZ/ZZ, or any non-MM).

```{r model selection results}
# read in model summary data
read_rds(here::here("data/aatd-1-copd.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_copd_res
read_rds(here::here("data/aatd-1-count.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_count
read_rds(here::here("data/aatd-1-metric.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_metric
read_rds(here::here("data/aatd-1-pred.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_pred
read_rds(here::here("data/aatd-1-fr-pred.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_fr_res_pred
# model family groups
read_rds(here::here("data/aatd-1-pred.rds")) %>%
  distinct(model) %>%
  mutate(type = case_when(
    model == "logistic regression" | model == "linear discriminant" ~
      "generalized linear",
    str_detect(model, "decision") | model == "random forest" ~
      "rule/tree-based",
    model == "nearest neighbor" ~ "distance-based",
    str_detect(model, "svm") ~ "kernel-based"
  )) %>%
  mutate(type = fct_inorder(type)) ->
  model_types
```

For this phase, each model family was fitted using a fixed set of parameter specifications (**need to make defaults explicit**):

* logistic regression: no penalty term
* linear discriminant analysis: no penalty term
* decision rules: 1 tree
* decision tree: depth 6
* random forest: 4 predictors, 120 trees
* nearest neighbor: 360 neighbors, triangular weights
* support vector machines: degree 1, 2, or 3
* FasterRisk: 7 terms, coefficient bound 5, beam search retention 12, ray search multipliers 24

Models were evaluated using a fixed sample of $\frac{1}{6}$ of the data.
The sample was stratified by a 4-way categorization of genotypes: ZZ, SZ, other non-MM, and MM. This was done in order to prevent significant changes in the balance of response values in each model.
Models were fit on a \frac{2}{3} training set of this sample and evaluated on the remaining \frac{1}{3} testing set. This partition was also stratified by genotype, and the same partition was used across all evaluations.
Since our goal in this phase was to identify generally over-performing models to optimize in the next phase, we used common or default parameter settings throughout.

## Performance curves

These are all binary classification problems, such that our probabilistic models can be used to define a sliding scale of screening thresholds based on different prescriptive trade-offs between the false negative and false positive rates.
As such, we primarily use performance curves to compare them.
The response variables are highly imbalanced, due to the rarity of abnormal genotypes.
We therefore take advantage of two popular performance curves: The receiver operator characteristic (ROC) and precision--recall (PR) curves.
These have complementary strengths and may reveal different trade-offs between the data (Davis & Goadrich, 2006).

These models are being designed as candidate substitutes for the current recommendation that patients be screened for an abnormal alpha-1 genotype if (and only if) they have been diagnosed with chronic obstructive pulmonary disease (COPD).
In addition to the models' performance curves, we also include a marker at the sensitivity and specificity of this recommendation for each combination of predictors response.
(The recommendation is not directly affected by the choice of predictors, but the data may differ slightly as the use of a new predictor forces us to discard cases for which its value is missing, which will slightly change the _measured_ sensitivity and specificity of the recommendation.)

### Receiver operator curves

#### Machine learning models

```{r roc plots for ML, fig.height=10}
aatd_mod_res_pred %>%
  left_join(model_types, by = "model") %>%
  filter(type != "rule/tree-based") %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_point(data = aatd_copd_res) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "bottom")
```

The ROC curves show LR and LDA to perform very similarly. This is to be expected, since LDA can be understood as a generalization of LR to more than two classes (though they are not equivalent in the two-class case).
NN models perform much less well, though still consistently better than DR, DT, and RF models, which are not visibly different from random screening. (For clarity, these have been omitted from the plot.)
In some cases, SVM models are competitive with LR and LDA, but in many others they perform worse than random screening.

#### Integer programming models

```{r roc plots for FR, fig.height=10}
aatd_fr_res_pred %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_point(data = aatd_copd_res) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "bottom")
```

FasterRisk returns an ensemble of models of similar accuracy, some of which may be more useful than others.
For each set of predictors and responses, we retrieved six models from the ensemble to compare.
At the task of predicting SZ or any abnormal genotype, these models performed equally well (or poorly). At the task of predicting ZZ genotype, two subgroups were distinguishable. One subgroup was superior with respect to AUROC overall but moreover showed greatest improvement at increasing sensitivity at low-to-medium specificity.

**Justify:**

```{r roc plots for best ML and FR, fig.height=10}
aatd_mod_res_pred %>%
  bind_rows(aatd_fr_res_pred) %>%
  filter(model == "logistic regression" | model == "nearest neighbor" |
           str_detect(model, "FasterRisk")) %>%
  mutate(type = ifelse(str_detect(model, "FasterRisk"), "FasterRisk", model)) %>%
  mutate(type = fct_inorder(type)) %>%
  group_by(model, type, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = type)) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_manual(values = c("#e41a1c", "#377eb8", "darkgrey")) +
  theme(legend.position = "bottom")
```

#### Sensitivity gain

The ROC curves suggest that ZZ and, less so, SZ/ZZ genotypes are significantly easier to predict than arbitrary abnormal genotypes, and they demonstrate a modest benefit to including smoking history in most models but an uncertain benefit to including gender.
As evaluated on our data, the standard recommendation underperforms random screening for SZ/ZZ and for arbitrary abnormal genotype, though it does provide a slight benefit in screening for ZZ.

Sensitivity is more valuable than specificity in our context: The potential cost of missing an abnormal AAT genotype is much higher than that of an unnecessary screen.
Our primary focus, then, is the top half-space of each ROC plot.
Troublingly, COPD-based screening shows lower sensitivity than random screening for all three abnormal genotype sets, though only slightly in the case of ZZ.
The NN and RF models show little difference from random screening toward the top-left corner of each plot, and the LR and LDA models show much clearer improvement with the addition of gender or smoking history as predictors and only minor (< 5%) improvement without.
The SVM models remain poor and inconsistent predictors when restricted to the high-sensitivity region.

The table and plot below summarize the sensitivity gains of all model families relative to random screening and to COPD-based screening, respectively.

```{r sensitivity gain, fig.height=6}
# improvement relative to random screening
aatd_mod_res_pred %>%
  bind_rows(aatd_fr_res_pred) %>%
  mutate(model = fct_inorder(str_remove(model, " [0-9]+$"))) %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  mutate(sensitivity_gain = sensitivity - 1 + specificity) %>%
  left_join(model_types, by = "model") %>%
  filter(response == "ZZ") %>%
  group_by(model, predictors) %>%
  filter(sensitivity_gain == max(sensitivity_gain)) %>%
  group_by(model) %>%
  filter(sensitivity_gain == min(sensitivity_gain) |
           sensitivity_gain == max(sensitivity_gain)) %>%
  mutate(bound = ifelse(
    sensitivity_gain == min(sensitivity_gain),
    "Minimum", "Maximum"
  )) %>%
  group_by(model, sensitivity_gain) %>%
  filter(.threshold == max(.threshold)) %>%
  slice_head(n = 1L) %>%
  ungroup() %>%
  mutate(bound = factor(bound, levels = c("Minimum", "Maximum"))) %>%
  select(
    Model = model,
    bound, `sensitivity gain` = sensitivity_gain
  ) %>%
  pivot_wider(names_from = bound, names_glue = "{bound} {.value}",
              values_from = `sensitivity gain`, names_sort = TRUE) %>%
  mutate(across(
    where(is.double),
    ~ str_c(format(round(., digits = 3L) * 100, nsmall = 1L, trim = TRUE), "%")
  )) %>%
  set_names(c(
    "Model",
    "Least peak sensitivity gain (ZZ)", "Greatest peak sensitivity gain (ZZ)"
  )) %>%
  knitr::kable(align = "lrr")
# improvement relative to COPD-based screening
aatd_mod_res_pred %>%
  bind_rows(aatd_fr_res_pred) %>%
  mutate(model = fct_inorder(str_remove(model, " [0-9]+$"))) %>%
  left_join(model_types, by = "model") %>%
  filter((type == "generalized linear" | model == "FasterRisk") &
           response == "ZZ") %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  rename_with(~ str_c("model_", .), c(sensitivity, specificity)) %>%
  left_join(aatd_copd_res, by = c("predictors", "response")) %>%
  rename_with(~ str_c("copd_", .), c(sensitivity, specificity)) %>%
  # sensitivity gain relative to COPD at minimum higher specificity
  mutate(
    model_copd_dist = abs(model_specificity - copd_specificity),
    model_copd_sign = sign(model_specificity - copd_specificity)
  ) %>%
  group_by(model, predictors, response, model_copd_sign) %>%
  filter(model_copd_dist == min(model_copd_dist)) %>%
  ungroup() %>%
  transmute(
    model, predictors, response, .threshold,
    specificity_diff = model_specificity - copd_specificity,
    sensitivity_diff = model_sensitivity - copd_sensitivity
  ) %>%
  unite("specification", predictors, response, sep = " -> ") %>%
  ggplot(aes(x = specificity_diff, y = sensitivity_diff,
             shape = model, color = specification)) +
  coord_equal() +
  geom_point() +
  geom_hline(yintercept = 0) + geom_vline(xintercept = 0) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  labs(x = "Change in specificity", y = "Change in sensitivity") +
  theme(legend.position = "right")
```

The greatest improvements in ZZ prediction over random screening are limited to the LR, LDA, and FR models and range from 25 to 45 percentage points.
These obtain near 50% specificity, which makes for useful comparisons against COPD-based screening.
For ZZ prediction (see the plot above), the LR, LDA, and FR models obtain between 19 and 35 percentage point improvement over COPD-based screening in sensitivity subject to a specificity loss of less than 5 percentage points.

### Precision--recall curves

Whereas recall is equivalent to sensitivity (true positive rate, the rate at which true cases test positive), precision is not the true negative rate (specificity) but the rate at which positive tests yield true cases (the positive predictive value, and the complement of the false discovery rate: $\text{PPV} = 1 - \text{FDR}$).
Precision--recall curves therefore map the cost, in decreased value of tests (upward along the ordinate), of increasing the detection of cases (rightward along the abscissa).
In the plots below, note that both recall and precision lie along log-transformed axes.

#### Machine learning models

```{r pr plots for ML, fig.height=10}
aatd_mod_res_pred %>%
  left_join(model_types, by = "model") %>%
  filter(type != "rule/tree-based") %>%
  group_by(model, predictors, response) %>%
  pr_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = recall, y = precision)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  # lims(x = c(0, 1), y = c(0, 1)) +
  scale_x_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_y_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "bottom")
```

The model families are not as clearly distinguishable in terms of precision and recall.
The superiority of LR and LDA is discernible as the curves surpass 1% recall: the NN curves fall consistently below. (If the DR, DT, and RF curves are plotted, they simply linearly interpolate between points at the top-left and bottom-right extremes, which gives a misleading idea of their performance in PR curves generally and for log--log plots.
When predicting ZZ, all curves attain peak precision between 1% and 10% recall and decline in precision as recall approaches 100%. (The pattern is similar for predicting SZ/ZZ.)
These differences are marginal, however---as can be seen when the PR curves are plotted on untransformed axes, all of these models perform quite poorly.

#### Integer programming models

```{r pr plots for FR, fig.height=10}
aatd_fr_res_pred %>%
  group_by(model, predictors, response) %>%
  pr_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = recall, y = precision)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  # lims(x = c(0, 1), y = c(0, 1)) +
  scale_x_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_y_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_color_brewer(type = "qual", guide = "none") +
  theme(legend.position = "bottom")
```

PR curves are less discriminating among the FasterRisk models.
Overall, the FR models are competitive with the best of the ML models.

Taken together, the ROC and PR curves favor logistic regression (and linear discriminant analysis) as candidate bases for a new risk score, though they should be carefully compared with the use-ready risk scores produced via integer programming by FasterRisk.

**Justify:**

```{r pr plots for best ML and FR, fig.height=10}
aatd_mod_res_pred %>%
  bind_rows(aatd_fr_res_pred) %>%
  filter(model == "logistic regression" | model == "nearest neighbor" |
           str_detect(model, "FasterRisk")) %>%
  mutate(type = ifelse(str_detect(model, "FasterRisk"), "FasterRisk", model)) %>%
  mutate(type = fct_inorder(type)) %>%
  group_by(model, type, predictors, response) %>%
  pr_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = recall, y = precision)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = type)) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  # lims(x = c(0, 1), y = c(0, 1)) +
  scale_x_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_y_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_color_manual(values = c("#e41a1c", "#377eb8", "darkgrey")) +
  theme(legend.position = "bottom")
```

# Phase 2: Model optimization

```{r model optimization results}
# read in machine learning evaluation data
read_rds(here::here("data/aatd-2-eval-ml.rds")) %>%
  mutate(across(c(model, predictors, response), fct_inorder)) %>%
  mutate(number = 1L) %>%
  unnest(hyperparameters) ->
  aatd_ml_metrics
read_rds(here::here("data/aatd-2-eval-fr.rds")) %>%
  select(-fold) %>%
  mutate(across(c(model, predictors, response), fct_inorder)) ->
  aatd_fr_metrics
```

We considered three model families for the optimization phase: logistic regression, random forest, and nearest neighbors.
We omitted linear discriminant analysis because it closely matched the performance of logistic regression, and because it is less well-understood by clinical researchers and not commonly used to derive risk scores. We also note that it relies on weaker assumptions about the data, which translate into less interpretable model components.
We omitted decision trees because they showed no benefit in any of the experiments in Phase 1.

Due to limitations of the implementation, nearest neighbors models were computationally expensie: Our data comprise binary (and, in some cases, categorical) variables, so that the space of possible records is a hypercube. As a result, only a small number of neighborhoods can be defined around each case, using the Hamming distance defined by the number of variables on which two cases differ, and records are duplicated at high rates. In contrast, most NN implementations rely on the computation of all pairwise distances between cases, which is highly redundant in this setting. Without a weighting scheme on the variables, the NN approach is mismatched, and would require a new implementation to be computationally feasible.

We omitted support vector machines in part because they proved extremely time- and memory-intensive to fit.
In general, SVMs are less interpretable than LR, decision tree, and NN models, and thereby less suitable for risk score derivation, but they can be justified when their predictive performance significantly exceeds these approaches. Our preliminary experiments showed no evidence of superior performance---indeed, SVMs performed slightly worse than most other model families---so we omit them from consideration.

As in the first phase, we included models obtained by FasterRisk in this comparison.
These models are generated and optimized in fundamentally different ways from the preceding model families:
First, the FasterRisk program returns a pool of near-optimal models rather than a single optimal model. This is in part because the underlying space of possible models is not approximately continuous but coarsely discrete: Whereas the coefficients of LR, the tree sizes and counts of RF, and the neighborhood sizes of NN can be continuously tuned, the integer coefficients of FR models cannot. Moreover, two similarly-performing risk scores may assign meaningfully different weights to the same or even different sets of predictors, so the selection of a "best" model from among this pool has less to do with a marginal improvement in some performance measure than with the usefulness of its interpretation.

## Comparison of model families

Across the same sets of predictors and responses as in Phase 1, we evaluated LR, RF, and NN models using 6-fold cross validation on a $\frac{4}{5}$ training set.
Within this set, for each 5--1 split of a 6-fold partition, models were _trained_ and _tuned_ on the $\frac{5}{6}$ part by identifying the parameter settings that obtained the best fit, then _tested_ on the remaining $\frac{1}{6}$.
Logistic regression models were tuned over 6 values of the penalty parameter ($10 ^ {-10}$ through $10 ^ 0$ with exponents at increments of $2$), which controls the cost of each additional predictor in the pruned model.
Random forest models were tuned over 10 parameter settings: each combination of 5 numbers of trees ($1$ through $100$ at rounded powers of $\sqrt{10}$) and 2 numbers of predictors per tree ($p ^ {1/3} \approx 3$ and $\sqrt{p} \approx 4$, where $p$ is the total number of predictors).
Nearest neighbors models were tuned over $4 \times 2 = 8$ parameter settings: neighborhood sizes of $10^k$ were considered for $k=1,2,3,4$, and two weighting schemes were considered (rectangular and triangular).

We tuned two parameter of FasterRisk: the number of terms (predictors) allowed in the score, and the bound on the values of their coefficients.
In the first phase, we allotted 7 terms to each FR model, a rough upper bound on the number of "slots" available to human working memory (Miller &al, 2018), and coefficient values between $-5$ and $5$, the default setting.
It may be that allocation of more predictors or greater coefficient range would achieve still superior performance, and many widely-used risk scores rely on larger values of both numbers.
We set out to determine whether this is the case.
The fewest predictors available were 18 (lung and liver history only), whereas the most convenient risk scores include 5 or so items. We obtained risk scores using 5, 10, and 15 items each.
We considered absolute coefficient bounds on either side of $5$, i.e. $3$ (yielding a Likert-style range from $-3$ to $3$) and $7$.
The beam search retention and ray search multiplier parameters were held fixed at 8 and 16, respectively (lower than in the first phase in order to reduce runtime).
For each combination of predictors, response, number of terms, and coefficient range, we sampled 2 generated risk scores.

We focused on two performance measures: overall accuracy and AUROC.
By far the strongest determinant of predictive performance was the genotype subclass being predicted: prediction of ZZ alone achieved greatest accuracy (near 99%), prediction of SZ or ZZ was nearly as accurate, and prediction of any abnormal (non-MM) genotype was much more difficult (near 85%).
This pattern was much noisier, though still detectable, when evaluating performance by AUROC---though by this metric the model family was most determinative (logistic regression outperformed random forest on every task.)
This may be due not to the task of predicting ZZ genotypes being easier but to the population of ZZ cases being smaller, which can increase the nominal performance of a model by increasing the lower bounds on its sensitivity ... (cite paper)
Though, since ZZ cases are also the most clinically urgent to detect, we focus on this task when evaluating the optimized models.

Multiple FR models are returned with each run, under the same parameter settings.
As with the ML models, we fit FR models on the $\frac{5}{6}$ training data and evaluate each model's performance on the $\frac{1}{6}$ testing data, which yields 12 rather than 6 model evaluations per parameter setting.

### Range of performance across parameter settings

The plot below locates the mean accuracy and mean AUROC of the families of fitted ZZ prediction models over their respective parameter settings.
The results are separated by model family, model specification (predictors used), and evaluation measure; each point is a mean of 6 (ML) or 12 (FR) within-fold evaluations.
The logistic regression models varied the penalty parameter while the random forest models varied the number of predictors used in each tree and the number of trees in the forest.

```{r performance across parameters, fig.height=4.5}
# compare performance of ZZ models across parameter settings
aatd_ml_metrics %>%
  bind_rows(aatd_fr_metrics) %>%
  filter(response == "ZZ") %>%
  mutate(model = fct_inorder(model)) %>%
  group_by_at(vars(-id, -number, -.estimate)) %>%
  summarize(mean = mean(.estimate), .groups = "drop") %>%
  filter(.metric == "accuracy" | .metric == "roc_auc") %>%
  mutate(.metric = c(accuracy = "Accuracy", roc_auc = "AUROC")[.metric]) %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_rev(formula)) %>%
  ggplot(aes(x = mean, y = formula)) +
  facet_grid(rows = vars(model), cols = vars(.metric), scales = "free_x") +
  geom_boxplot(color = "#d32737") +
  geom_jitter(width = 0, height = 1/4, alpha = .5) +
  labs(x = NULL, y = NULL) +
  ggtitle("Performance of predictive models of ZZ genotype")
```

Measured by overall accuracy, LR models showed more consistency across parameter settings, but some RF models achieved greater ultimate performance than all but an outlier LR model. NN models showed greater variability and competitive performance.
Most FR models improved upon all three ML models, but with variability similar to that of NN models.
On the more important measure of AUROC, however, LR models universally outperformed RF and NN models, and even outperformed most FR models.
This is not unexpected; FR models are derived from LR models and comprise a subclass subject to additional constraints. But the narrow margins by which LR models outperform (some) FR models suggest that they are a superior approach. (Optimized LR models must be transformed into usable risk scores, at some cost to their performance that cannot be known in advance.)

Among the LR models, the inclusion of gender did not improve performance by either measure, but the inclusion of smoking history led to consistent, though not highly significant, improvement (a less than 0.01% increase in accuracy and a less than 5% increase in AUROC).
Among the FR models, only use history (not exposure history) provided substantial gains in performance over the history-only variable set.

### Optimized parameter settings

```{r optimal parameters}
# best parameter settings for each model and formula by two measures
aatd_ml_metrics %>%
  bind_rows(aatd_fr_metrics) %>%
  filter(response == "ZZ") %>%
  mutate(model = fct_inorder(model)) %>%
  group_by_at(vars(-id, -number, -.estimate)) %>%
  summarize(mean = mean(.estimate), sd = sd(.estimate), .groups = "drop") %>%
  filter(.metric == "accuracy" | .metric == "roc_auc") %>%
  mutate(.metric = c(accuracy = "Accuracy", roc_auc = "AUROC")[.metric]) %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_rev(formula)) %>%
  group_by(model, predictors, response, .metric) %>%
  filter(mean == max(mean)) %>%
  # rank model parameter settings
  mutate(param_pref = case_when(
    ! is.na(penalty) ~ order(desc(penalty)),
    ! is.na(trees) & ! is.na(mtry) ~ order(trees, mtry),
    ! is.na(neighbors) ~ order(neighbors),
    ! is.na(terms) & ! is.na(bound) ~ order(terms, bound)
  )) %>%
  filter(param_pref == min(param_pref)) %>%
  ungroup() ->
  aatd_opt
```

Below we inspect the settings in each model that achieved the greatest performance, by overall accuracy and by the area under the receiver operator characteristic curve (AUROC):

```{r optimal performance table}
aatd_opt %>%
  mutate(
    penalty = ifelse(
      is.na(penalty), NA_character_,
      str_replace(format(penalty, scientific = TRUE), "e", " Ã— 10^")
    ),
    mtry = as.character(mtry), trees = as.character(trees)
  ) %>%
  mutate(across(
    c(penalty, mtry, trees, terms),
    ~ ifelse(is.na(.), "&ndash;", .)
  )) %>%
  mutate(mean = format(round(mean, digits = 3L), digits = 3L, nsmall = 1L)) %>%
  select(
    Measure = .metric, Model = model, Predictors = predictors,
    penalty, mtry, trees, terms, bound,
    `Mean performance` = mean
  ) %>%
  arrange(Measure, Model, Predictors) %>%
  knitr::kable(align = "lllrrrrr")
```

While high accuracy tends to be achieved by more parsimonious models, overall performance as measured by AUROC is improved with larger models.
It is likely that the high accuracy rates are largely artifacts of the imbalance in the data; the ROC curves provide a more discriminating measure of performance. These are used below to visually compare performance between the model families and specifications.
Remember that performance in each setting is an average taken over folded cross-validation, from which we derive compatibility intervals.

```{r optimal performance plot, fig.height=4}
aatd_opt %>%
  filter(.metric == "AUROC") %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_rev(formula)) %>%
  ggplot(aes(x = mean, xmin = mean - 2*sd, xmax = mean + 2*sd, y = formula)) +
  facet_grid(model ~ .) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  geom_pointrange() +
  labs(x = "AUROC", y = NULL)
```

Logistic regression models clearly achieve greater performance in this setting than random forest models, which are only marginally (though detectably) better than chance.
Meanwhile, FasterRisk yields models that are competitive with LR models, in the sense that FR may be expected to outperform LR on new data much (though not necessarily most) of the time.
Because they are already interpretable, i believe FR models provide the most promising family for the final risk score.

### Sensitivity of FasterRisk models

While the 15-term, large-coefficient FasterRisk models performed best, if performance is minimally impacted by tightening these allowances, then the gain to interpretability and usability may be worth it.
Here we compare all nine FasterRisk parameter settings on the prediction of the ZZ genotype.
On each fold, two models were obtained, yielding $6 \times 2 = 12$ models per parameter setting.

```{r FasterRisk performance sensitivity, fig.height=4}
aatd_fr_metrics %>%
  filter(response == "ZZ" & .metric == "roc_auc") %>%
  mutate(model = fct_inorder(model)) %>%
  group_by_at(vars(-id, -number, -.estimate)) %>%
  summarize(mean = mean(.estimate), se = sd(.estimate) / sqrt(n())) %>%
  ungroup() %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_inorder(fct_drop(formula))) %>%
  mutate(hyperparameters = str_c("terms = ", terms, ", bound = ", bound)) %>%
  mutate(hyperparameters = fct_rev(fct_inorder(hyperparameters))) %>%
  ggplot(aes(x = mean, xmin = mean - 2*se, xmax = mean + 2*se,
             y = hyperparameters)) +
  facet_wrap(facets = vars(formula)) +
  # geom_vline(xintercept = 0.5, linetype = "dashed") +
  geom_pointrange() +
  labs(x = "AUROC", y = NULL)
```

It appears that the number of terms is more consequential than the point bounds, though we compared models using a smaller spread of the latter. (The bounds seem more important when predicting genotype either SZ or ZZ in contrast to others.)
We see improvements of between $0.012$ and $0.035$ from the strictest to the loosest parameter settings.
We will need to decide whether additional terms or larger scores are warranted for a roughly 2% gain in AUROC---though the gains are smaller when using only medical history to predict genotype.

# Phase 3: Model evaluation

## Selection of optimized model

The only additional predictor that improves both LR and FR models is smoking history, and the improvement appears to be substantial.
This could be carefully tested in the LR models via a likelihood ratio test of the null hypothesis that the model with smoking history outperforms the model without, or by using the Akaike Information Criterion to quantify the trade-off between predictive performance and model complexity.
The answer is less straightforward for the FR models.

Finally, whether a LR or FR model is selected for the final risk score, we must decide how sparse to require it. This amounts to choosing a complexity penalty for the LR model or to choosing a term limit and coefficient range for the FR model.

## Obtaining risk scores from optimized models

FasterRisk provides ready-made risk scores; no further processing needs to be done.
Risk scores can be calculated by adding small integer multiples of the categorical attributes and then mapping these scores to their associated risk estimates (probabilities of abnormal genotype).

The linear regression model will undergo the following steps, adapted from Sullivan &al (2004):

1. Reference values will be defined for all predictors (for medical history, these will be 0, i.e. no diagnosis). A baseline risk estimate will be established for cases that take all reference values. (This corresponds to the intercept in the logistic regression model.)
2. A maximum point value for any predictor will be decided. The tentative choice is 25. This will cap the value of the coefficients used in the calculation of the risk score. Larger values will allow more predictors to appear in the risk score and more granularity in its risk estimates; smaller values will make calculations easier.
3. All coefficients will be multiplied and rounded, so that the largest value is the maximum point value. This obtains the point values. Predictors whose coefficients round to zero will be excluded from the risk score.
4. Each score $s$ (the point total) will be mapped to a risk estimate $p$ (a probability) via the link function that defines the logistic regression model:
$$p = \frac{1}{1 + \exp(s)}$$

## Final evaluation

The selected model family and hyperparameters will be evaluated by first fitting them to the entire $\frac{4}{5}$ training set from the second phase and then evaluating its predictions on the remaining $\frac{1}{5}$ testing set.
This will provide our best estimate for the performance of the final model on new data (of the same type as the data already collected). Remember that this is, by definition, data we do not currently have.

The final model will then be obtained by fitting the same family and hyperparameters to _the entire data set_. This final model cannot be evaluated using our data but will provide our best proposal for a general risk score.
