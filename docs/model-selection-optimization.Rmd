---
title: "Selection and Optimization of Predictive Models of AAT Genotype"
author: "Jason Cory Brunson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      fig.width = 8)
library(tidyverse)
library(lubridate)
library(tidymodels)
source(here::here("code/settings.r"))
theme_set(theme_bw())
```

This phase of the project aims to complete two goals:

1. Determine the most effective model family to predict AAT genotype from lung and liver medical history.
2. Identify parameter settings for which this model family achieves the greatest predictive accuracy.

The goal of the next phase will be to define a clinically practical risk score for abnormal AAT genotype based on the optimized model.

**Note:** The document is being revised to reflect changes made to the pre-processing phase. In particular, patient-reported AATD and history of liver transplantation have been removed, as we view these as determinative indicators that a patient's AAT genotype is known or unambiguously should be tested. Also, patient-reported COPD, emphysema, and bronchitis have been combined into a single COPD variable, which we view as more robust and meaningful than three separate indicators.

# Phase 1: Model selection

We considered several families of predictive model: logistic regression (LR), linear discriminant analysis (LDA), decision rules (DR), decision tree (DT), random forest (RF), nearest neighbors (NN), and support vector machine (SVM) using 1-, 2-, and 3-degree polynomial specifications.
We also considered integer risk scores obtained using FasterRisk (FR), which would not need to be transformed in order to be clinically practical (Liu &al, 2022).
We will compare these models over a $3 \times 3$ grid of prediction tasks given by each combination of predictors (lung/liver medical history only, together with gender, and together with either smoking exposure or smoking history) and of response (ZZ, SZ/ZZ, or any non-MM).

```{r model selection results}
# read in model summary data
read_rds(here::here("data/aatd-1-copd.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_copd_res
read_rds(here::here("data/aatd-1-count.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_count
read_rds(here::here("data/aatd-1-metric.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_metric
read_rds(here::here("data/aatd-1-pred.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_mod_res_pred
read_rds(here::here("data/aatd-1-fr-pred.rds")) %>%
  filter(predictors != "Dx+age") %>%
  mutate(across(c(predictors, response), fct_inorder)) ->
  aatd_fr_res_pred
# model family groups
read_rds(here::here("data/aatd-1-pred.rds")) %>%
  distinct(model) %>%
  mutate(type = case_when(
    model == "logistic regression" | model == "linear discriminant" ~
      "generalized linear",
    str_detect(model, "decision") | model == "random forest" ~
      "rule/tree-based",
    model == "nearest neighbor" ~ "distance-based",
    str_detect(model, "svm") ~ "kernel-based"
  )) %>%
  mutate(type = fct_inorder(type)) ->
  model_types
```

For this phase, each model family was fitted using a fixed set of parameter specifications (**need to make defaults explicit**):

* logistic regression: no penalty term
* linear discriminant analysis: no penalty term
* decision rules: 1 tree
* decision tree: depth 7
* random forest: 4 predictors, 120 trees
* nearest neighbor: 360 neighbors, triangular weights
* support vector machines: degree 1, 2, or 3
* FasterRisk: 7 terms, coefficient bound 5, beam search retention 12, ray search multipliers 24

Models were evaluated using a fixed sample of $\frac{1}{6}$ of the data.
The sample was stratified by a 4-way categorization of genotypes: ZZ, SZ, other non-MM, and MM. This was done in order to prevent significant changes in the balance of response values in each model.
Models were fit on a \frac{2}{3} training set of this sample and evaluated on the remaining \frac{1}{3} testing set. This partition was also stratified by genotype, and the same partition was used across all evaluations.
Since our goal in this phase was to identify generally over-performing models to optimize in the next phase, we used common or default parameter settings throughout.

## Performance curves

These are all binary classification problems, such that our probabilistic models can be used to define a sliding scale of screening thresholds based on different prescriptive trade-offs between the false negative and false positive rates.
As such, we primarily use performance curves to compare them.
The response variables are highly imbalanced, due to the rarity of abnormal genotypes.
We therefore take advantage of two popular performance curves: The receiver operator characteristic (ROC) and precision--recall (PR) curves.
These have complementary strengths and may reveal different trade-offs between the data (Davis & Goadrich, 2006).

These models are being designed as candidate substitutes for the current recommendation that patients be screened for an abnormal alpha-1 genotype if (and only if) they have been diagnosed with chronic obstructive pulmonary disease (COPD).
In addition to the models' performance curves, we also include a marker at the sensitivity and specificity of this recommendation for each combination of predictors response.
(The recommendation is not directly affected by the choice of predictors, but the data may differ slightly as the use of a new predictor forces us to discard cases for which its value is missing, which will slightly change the _measured_ sensitivity and specificity of the recommendation.)

### Receiver operator curves

#### Machine learning models

```{r roc plots for ML, fig.height=10}
aatd_mod_res_pred %>%
  left_join(model_types, by = "model") %>%
  filter(type != "rule/tree-based") %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_point(data = aatd_copd_res) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "bottom")
```

The ROC curves show the linear models LR and LDA to perform very similarly. This is to be expected, since LDA can be understood as a generalization of LR to more than two classes (though they are not equivalent in the two-class case).
NN models perform much less well, though still consistently better than the rule- and tree-based models DR, DT, and RF, which are not visibly different from random screening. (For clarity, these have been omitted from the plot.)
In some cases, the SVM models are competitive with LR and LDA, but in several others they perform worse than random screening.

#### Integer programming models

```{r roc plots for FR, fig.height=10}
aatd_fr_res_pred %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_point(data = aatd_copd_res) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "bottom")
```

FasterRisk returns an ensemble of models of similar performance in most cases, some of which may be more useful than others.
For each set of predictors and responses, we retrieved six models from the ensemble to compare.
At the task of predicting SZ or any abnormal genotype, these models performed equally well (or poorly); at the task of predicting ZZ genotype, they were clearly inferior to the linear and distance-based models.

Below, we superimpose ROC curves for the collections of FR models and for the LR and NN models in order to better compare their performance.
While most FR models underperform LR when predicting ZZ, especially from only medical history, they become more competitive as all models weaken on the other predictive tasks.

```{r roc plots for best ML and FR, fig.height=10}
aatd_mod_res_pred %>%
  bind_rows(aatd_fr_res_pred) %>%
  filter(model == "logistic regression" | model == "nearest neighbor" |
           str_detect(model, "FasterRisk")) %>%
  mutate(type = ifelse(str_detect(model, "FasterRisk"), "FasterRisk", model)) %>%
  mutate(type = fct_inorder(type)) %>%
  group_by(model, type, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = type)) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_manual(values = c("#e41a1c", "#377eb8", "darkgrey")) +
  theme(legend.position = "bottom")
```

#### Sensitivity gain

The ROC curves suggest that more precise abnormal genotypes are significantly easier to predict, and they demonstrate a modest benefit to including smoking history in most models.
As evaluated on our data, the standard recommendation underperforms random screening for SZ/ZZ and for arbitrary abnormal genotype, though it does provide a slight benefit in screening for ZZ.

Sensitivity is more valuable than specificity in our context: The potential cost of missing an abnormal AAT genotype is much higher than that of an unnecessary screen, and the primary goal of a new screening recommendation will be to successfully identify more abnormal genotypes.
Our primary focus, then, is the top half-space of each ROC plot.
Troublingly, COPD-based screening shows lower sensitivity than random screening for all three abnormal genotype sets, though only slightly in the case of ZZ.
The NN and RF models show little difference from random screening toward the top-left corner of each plot, and the LR and LDA models show much clearer improvement with the addition of gender or smoking history as predictors and only minor (< 5%) improvement without.
The SVM models remain poor and inconsistent predictors when restricted to the high-sensitivity region.

The table and plot below summarize the sensitivity gains of all model families relative to random screening and to COPD-based screening, respectively.

```{r sensitivity gain, fig.height=6}
# improvement relative to random screening
aatd_mod_res_pred %>%
  bind_rows(aatd_fr_res_pred) %>%
  mutate(model = fct_inorder(str_remove(model, " [0-9]+$"))) %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  mutate(sensitivity_gain = sensitivity - 1 + specificity) %>%
  left_join(model_types, by = "model") %>%
  filter(response == "ZZ") %>%
  group_by(model, predictors) %>%
  filter(sensitivity_gain == max(sensitivity_gain)) %>%
  group_by(model) %>%
  filter(sensitivity_gain == min(sensitivity_gain) |
           sensitivity_gain == max(sensitivity_gain)) %>%
  mutate(bound = ifelse(
    sensitivity_gain == min(sensitivity_gain),
    "Minimum", "Maximum"
  )) %>%
  group_by(model, sensitivity_gain) %>%
  filter(.threshold == max(.threshold)) %>%
  slice_head(n = 1L) %>%
  ungroup() %>%
  mutate(bound = factor(bound, levels = c("Minimum", "Maximum"))) %>%
  select(
    Model = model,
    bound, `sensitivity gain` = sensitivity_gain
  ) %>%
  pivot_wider(names_from = bound, names_glue = "{bound} {.value}",
              values_from = `sensitivity gain`, names_sort = TRUE) %>%
  mutate(across(
    where(is.double),
    ~ str_c(format(round(., digits = 3L) * 100, nsmall = 1L, trim = TRUE), "%")
  )) %>%
  set_names(c(
    "Model",
    "Least peak sensitivity gain (ZZ)", "Greatest peak sensitivity gain (ZZ)"
  )) %>%
  knitr::kable(align = "lrr")
# improvement relative to COPD-based screening
aatd_mod_res_pred %>%
  bind_rows(aatd_fr_res_pred) %>%
  mutate(model = fct_inorder(str_remove(model, " [0-9]+$"))) %>%
  left_join(model_types, by = "model") %>%
  filter((type == "generalized linear" | model == "FasterRisk") &
           response == "ZZ") %>%
  group_by(model, predictors, response) %>%
  roc_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ungroup() %>%
  rename_with(~ str_c("model_", .), c(sensitivity, specificity)) %>%
  left_join(aatd_copd_res, by = c("predictors", "response")) %>%
  rename_with(~ str_c("copd_", .), c(sensitivity, specificity)) %>%
  # sensitivity gain relative to COPD at minimum higher specificity
  mutate(
    model_copd_dist = abs(model_specificity - copd_specificity),
    model_copd_sign = sign(model_specificity - copd_specificity)
  ) %>%
  group_by(model, predictors, response, model_copd_sign) %>%
  filter(model_copd_dist == min(model_copd_dist)) %>%
  ungroup() %>%
  transmute(
    model, predictors, response, .threshold,
    specificity_diff = model_specificity - copd_specificity,
    sensitivity_diff = model_sensitivity - copd_sensitivity
  ) %>%
  unite("specification", predictors, response, sep = " -> ") %>%
  ggplot(aes(x = specificity_diff, y = sensitivity_diff,
             shape = model, color = specification)) +
  coord_equal() +
  geom_point() +
  geom_hline(yintercept = 0) + geom_vline(xintercept = 0) +
  scale_x_continuous(labels = scales::label_percent()) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  labs(x = "Change in specificity", y = "Change in sensitivity") +
  theme(legend.position = "right")
```

The greatest improvements in ZZ prediction over random screening are limited to the LR, LDA, NN, SVM, and FR models and range from 13 to 29 percentage points; though these improvements are much less consistent for the SVM models.
These obtain near 50% specificity, which makes for useful comparisons against COPD-based screening.
For ZZ prediction (see the plot), the LR, LDA, and FR models obtain up to a 12 percentage point improvement over COPD-based screening in sensitivity subject to a specificity loss of less than 5 percentage points.

### Precision--recall curves

Whereas recall is equivalent to sensitivity (true positive rate, the rate at which true cases test positive), precision is not the true negative rate (specificity) but the rate at which positive tests yield true cases (the positive predictive value, and the complement of the false discovery rate: $\text{PPV} = 1 - \text{FDR}$).
Precision--recall curves therefore map the cost, in decreased value of tests (upward along the ordinate), of increasing the detection of cases (rightward along the abscissa).
In the plots below, note that both recall and precision lie along log-transformed axes.

#### Machine learning models

```{r pr plots for ML, fig.height=10}
aatd_mod_res_pred %>%
  left_join(model_types, by = "model") %>%
  filter(type != "rule/tree-based") %>%
  group_by(model, predictors, response) %>%
  pr_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = recall, y = precision)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  # lims(x = c(0, 1), y = c(0, 1)) +
  scale_x_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_y_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_color_brewer(type = "qual") +
  theme(legend.position = "bottom")
```

The model families are not as clearly distinguishable in terms of precision and recall.
The superiority of LR and LDA is discernible as the curves surpass 1% recall: the NN curves fall consistently below. (If the DR, DT, and RF curves are plotted, they simply linearly interpolate between points at the top-left and bottom-right extremes, which gives a misleading idea of their performance in PR curves generally and for log--log plots.
When predicting ZZ, all curves attain peak precision between 1% and 20% recall and decline in precision as recall approaches 100%. (The pattern is similar for predicting SZ/ZZ.)
These differences are marginal, however---as can be seen when the PR curves are plotted on untransformed axes, all of these models perform quite poorly.

#### Integer programming models

```{r pr plots for FR, fig.height=10}
aatd_fr_res_pred %>%
  group_by(model, predictors, response) %>%
  pr_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = recall, y = precision)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = model)) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  # lims(x = c(0, 1), y = c(0, 1)) +
  scale_x_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_y_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_color_brewer(type = "qual", guide = "none") +
  theme(legend.position = "bottom")
```

PR curves are less discriminating among the FasterRisk models.
Overall, the FR models are competitive with the best of the ML models.
Below the LR and NN models are again compared to the collections of FR models.

```{r pr plots for best ML and FR, fig.height=10}
aatd_mod_res_pred %>%
  bind_rows(aatd_fr_res_pred) %>%
  filter(model == "logistic regression" | model == "nearest neighbor" |
           str_detect(model, "FasterRisk")) %>%
  mutate(type = ifelse(str_detect(model, "FasterRisk"), "FasterRisk", model)) %>%
  mutate(type = fct_inorder(type)) %>%
  group_by(model, type, predictors, response) %>%
  pr_curve(truth = geno_class, estimate = .pred_Abnormal) %>%
  ggplot(aes(x = recall, y = precision)) +
  facet_grid(rows = vars(predictors), cols = vars(response)) +
  coord_equal() +
  geom_path(aes(color = type)) +
  geom_point(data = filter(aatd_copd_res, predictors != "Dx+age")) +
  # lims(x = c(0, 1), y = c(0, 1)) +
  scale_x_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_y_continuous(trans = "log", breaks = breaks_log(base = 10),
                     labels = scales::label_percent()) +
  scale_color_manual(values = c("#e41a1c", "#377eb8", "darkgrey")) +
  theme(legend.position = "bottom")
```

Taken together, the ROC and PR curves favor logistic regression (and linear discriminant analysis) as candidate bases for a new risk score, though they should be carefully compared with the use-ready risk scores produced via integer programming by FasterRisk.

# Phase 2: Model optimization

```{r model optimization results}
# read in machine learning evaluation data
read_rds(here::here("data/aatd-2-eval-ml.rds")) %>%
  mutate(across(c(model, predictors, response), fct_inorder)) %>%
  mutate(number = 1L) %>%
  unnest(hyperparameters) ->
  aatd_ml_metrics
read_rds(here::here("data/aatd-2-eval-fr.rds")) %>%
  select(-fold) %>%
  mutate(across(c(model, predictors, response), fct_inorder)) ->
  aatd_fr_metrics
```

We considered three model families for the optimization phase: logistic regression, random forest, and nearest neighbors.
We omitted linear discriminant analysis because it closely matched the performance of logistic regression, and because it is less well-understood by clinical researchers and not commonly used to derive risk scores. We also note that it relies on weaker assumptions about the data, which translate into less interpretable model components.
We omitted decision trees because they showed no benefit in any of the experiments in Phase 1.

Due to limitations of the implementation, nearest neighbors models were computationally expensie: Our data comprise binary (and, in some cases, categorical) variables, so that the space of possible records is a hypercube. As a result, only a small number of neighborhoods can be defined around each case, using the Hamming distance defined by the number of variables on which two cases differ, and records are duplicated at high rates. In contrast, most NN implementations rely on the computation of all pairwise distances between cases, which is highly redundant in this setting. Without a weighting scheme on the variables, the NN approach is mismatched, and would require a new implementation to be computationally feasible.

We omitted support vector machines in part because they proved extremely time- and memory-intensive to fit.
In general, SVMs are less interpretable than LR, decision tree, and NN models, and thereby less suitable for risk score derivation, but they can be justified when their predictive performance significantly exceeds these approaches. Our preliminary experiments showed no evidence of superior performance---indeed, SVMs performed slightly worse than most other model families---so we omit them from consideration.

As in the first phase, we included models obtained by FasterRisk in this comparison.
These models are generated and optimized in fundamentally different ways from the preceding model families:
First, the FasterRisk program returns a pool of near-optimal models rather than a single optimal model. This is in part because the underlying space of possible models is not approximately continuous but coarsely discrete: Whereas the coefficients of LR, the tree sizes and counts of RF, and the neighborhood sizes of NN can be continuously tuned, the integer coefficients of FR models cannot. Moreover, two similarly-performing risk scores may assign meaningfully different weights to the same or even different sets of predictors, so the selection of a "best" model from among this pool has less to do with a marginal improvement in some performance measure than with the usefulness of its interpretation.

## Comparison of model families

Across the same sets of predictors and responses as in Phase 1, we evaluated LR, RF, and NN models using 6-fold cross validation on a $\frac{4}{5}$ training set.
Within this set, for each 5--1 split of a 6-fold partition, models were _trained_ and _tuned_ on the $\frac{5}{6}$ part by identifying the parameter settings that obtained the best fit, then _tested_ on the remaining $\frac{1}{6}$.
Logistic regression models were tuned over 6 values of the penalty parameter ($10 ^ {-10}$ through $10 ^ 0$ with exponents at increments of $2$), which controls the cost of each additional predictor in the pruned model.
Random forest models were tuned over 10 parameter settings: each combination of 5 numbers of trees ($1$ through $100$ at rounded powers of $\sqrt{10}$) and 2 numbers of predictors per tree ($p ^ {1/3} \approx 3$ and $\sqrt{p} \approx 4$, where $p$ is the total number of predictors).
Nearest neighbors models were tuned over $4 \times 2 = 8$ parameter settings: neighborhood sizes of $10^k$ were considered for $k=1,2,3,4$, and two weighting schemes were considered (rectangular and triangular).

We tuned two parameter of FasterRisk: the number of terms (predictors) allowed in the score, and the bound on the values of their coefficients.
In the first phase, we allotted 7 terms to each FR model, a rough upper bound on the number of "slots" available to human working memory (Miller &al, 2018), and coefficient values between $-5$ and $5$, the default setting.
It may be that allocation of more predictors or greater coefficient range would achieve still superior performance, and many widely-used risk scores rely on larger values of both numbers.
We set out to determine whether this is the case.
The fewest predictors available were 18 (lung and liver history only), whereas the most convenient risk scores include 5 or so items. We obtained risk scores using 5, 10, and 15 items each.
We considered absolute coefficient bounds on either side of $5$, i.e. $3$ (yielding a Likert-style range from $-3$ to $3$) and $7$.
The beam search retention and ray search multiplier parameters were held fixed at 8 and 16, respectively (lower than in the first phase in order to reduce runtime).
For each combination of predictors, response, number of terms, and coefficient range, we sampled 2 generated risk scores.

We focused on two performance measures: overall accuracy and AUROC.
By far the strongest determinant of predictive performance was the genotype subclass being predicted: prediction of ZZ alone achieved greatest accuracy (near 99%), prediction of SZ or ZZ was nearly as accurate, and prediction of any abnormal (non-MM) genotype was much more difficult (near 85%).
This pattern was much noisier, though still detectable, when evaluating performance by AUROC---though by this metric the model family was most determinative (logistic regression outperformed random forest on every task.)
This may be due not to the task of predicting ZZ genotypes being easier but to the population of ZZ cases being smaller, which can increase the nominal performance of a model by increasing the lower bounds on its sensitivity ... (cite paper)
Though, since ZZ cases are also the most clinically urgent to detect, we focus on this task when evaluating the optimized models.

Multiple FR models are returned with each run, under the same parameter settings.
As with the ML models, we fit FR models on the $\frac{5}{6}$ training data and evaluate each model's performance on the $\frac{1}{6}$ testing data, which yields 12 rather than 6 model evaluations per parameter setting.

### Range of performance across parameter settings

The plot below locates the mean accuracy and mean AUROC of the families of fitted ZZ prediction models over their respective parameter settings.
The results are separated by model family, model specification (predictors used), and evaluation measure; each point is a mean of 6 (ML) or 12 (FR) within-fold evaluations.
The logistic regression models varied the penalty parameter while the random forest models varied the number of predictors used in each tree and the number of trees in the forest.

```{r performance across parameters, fig.height=4.5}
# compare performance of ZZ models across parameter settings
aatd_ml_metrics %>%
  bind_rows(aatd_fr_metrics) %>%
  filter(response == "ZZ") %>%
  mutate(model = fct_inorder(model)) %>%
  group_by_at(vars(-id, -number, -.estimate)) %>%
  summarize(mean = mean(.estimate), .groups = "drop") %>%
  filter(.metric == "accuracy" | .metric == "roc_auc") %>%
  mutate(.metric = c(accuracy = "Accuracy", roc_auc = "AUROC")[.metric]) %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_rev(formula)) %>%
  ggplot(aes(x = mean, y = formula)) +
  facet_grid(rows = vars(model), cols = vars(.metric), scales = "free_x") +
  geom_boxplot(color = "#d32737") +
  geom_jitter(width = 0, height = 1/4, alpha = .5) +
  labs(x = NULL, y = NULL) +
  ggtitle("Performance of predictive models of ZZ genotype")
```

Measured by overall accuracy, most  models were consistent across parameter settings and with each other, with the exception that RF models in one instance overtook the others and in many instances fell behind (though note the very fine scale on which they differ).
Measured by area under the ROC curve, LR models showed highest performance and remarkable consistency across parameter settings, RF models performed consistently poorly, and NN models showed greater variability and intermediate competitive performance.
Most FR models improved upon RF and NN models but underperformed LR models.
This is not unexpected; FR models are derived from LR models and comprise a subclass subject to additional constraints. But the narrow margins by which LR models outperform (some) FR models suggest that either may be the best approach. (Optimized LR models must be transformed into usable risk scores, at some cost to their performance that cannot be known in advance.)

Among the LR models, the inclusion of gender did not improve performance by either measure, but the inclusion of smoking history led to consistent and detectable improvement, though a less than 0.01% increase in accuracy and a less than 5% increase in AUROC.
Among the FR models, only use history (not exposure history) provided substantial gains in performance over the history-only variable set.

### Optimized parameter settings

```{r optimal parameters}
# best parameter settings for each model and formula by two measures
aatd_ml_metrics %>%
  bind_rows(aatd_fr_metrics) %>%
  filter(response == "ZZ") %>%
  mutate(model = fct_inorder(model)) %>%
  group_by_at(vars(-id, -number, -.estimate)) %>%
  summarize(mean = mean(.estimate), sd = sd(.estimate), .groups = "drop") %>%
  filter(.metric == "accuracy" | .metric == "roc_auc") %>%
  mutate(.metric = c(accuracy = "Accuracy", roc_auc = "AUROC")[.metric]) %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_rev(formula)) %>%
  group_by(model, predictors, response, .metric) %>%
  filter(mean == max(mean)) %>%
  # rank model parameter settings
  mutate(param_pref = case_when(
    ! is.na(penalty) ~ order(desc(penalty)),
    ! is.na(trees) & ! is.na(mtry) ~ order(trees, mtry),
    ! is.na(neighbors) ~ order(neighbors),
    ! is.na(terms) & ! is.na(bound) ~ order(terms, bound)
  )) %>%
  filter(param_pref == min(param_pref)) %>%
  ungroup() ->
  aatd_opt
```

Below we inspect the settings in each model that achieved the greatest performance, by overall accuracy and by the area under the receiver operator characteristic curve (AUROC):

```{r optimal performance table}
aatd_opt %>%
  mutate(
    penalty = ifelse(
      is.na(penalty), NA_character_,
      str_replace(format(penalty, scientific = TRUE), "e", " × 10^")
    ),
    mtry = as.character(mtry), trees = as.character(trees)
  ) %>%
  mutate(across(
    c(penalty, mtry, trees, terms),
    ~ ifelse(is.na(.), "&ndash;", .)
  )) %>%
  mutate(mean = format(round(mean, digits = 3L), digits = 3L, nsmall = 1L)) %>%
  select(
    Measure = .metric, Model = model, Predictors = predictors,
    penalty, mtry, trees, terms, bound,
    `Mean performance` = mean
  ) %>%
  arrange(Measure, Model, Predictors) %>%
  knitr::kable(align = "lllrrrrr")
```

While high accuracy tends to be achieved by more parsimonious models, overall performance as measured by AUROC is improved with larger models.
It is likely that the high accuracy rates are largely artifacts of the imbalance in the data; the ROC curves provide a more discriminating measure of performance. These are used below to visually compare performance between the model families and specifications.
Remember that performance in each setting is an average taken over folded cross-validation, from which we derive compatibility intervals.

```{r optimal performance plot, fig.height=4}
aatd_opt %>%
  filter(.metric == "AUROC") %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_rev(formula)) %>%
  ggplot(aes(x = mean, xmin = mean - 2*sd, xmax = mean + 2*sd, y = formula)) +
  facet_grid(model ~ .) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  geom_pointrange() +
  labs(x = "AUROC", y = NULL)
```

Logistic regression models clearly achieve greater performance in this setting than random forest models, which are only marginally (though detectably) better than chance, and than nearest neighbor models.
Meanwhile, FasterRisk yields models that are competitive with LR models, in the sense that FR may be expected to outperform LR on new data much (though not necessarily most) of the time.
Because they are already interpretable, i believe FR models provide the most promising family for the final risk score.

### Sensitivity of FasterRisk models

While the 15-term, large-coefficient FasterRisk models performed best, if performance is minimally impacted by tightening these allowances, then the gain to interpretability and usability may be worth it.
Here we compare all nine FasterRisk parameter settings on the prediction of the ZZ genotype.
On each fold, two models were obtained, yielding $6 \times 2 = 12$ models per parameter setting.

```{r FasterRisk performance sensitivity, fig.height=4}
aatd_fr_metrics %>%
  filter(response == "ZZ" & .metric == "roc_auc") %>%
  mutate(model = fct_inorder(model)) %>%
  group_by_at(vars(-id, -number, -.estimate)) %>%
  summarize(mean = mean(.estimate), se = sd(.estimate) / sqrt(n())) %>%
  ungroup() %>%
  mutate(formula = interaction(predictors, response, sep = " -> ")) %>%
  mutate(formula = fct_inorder(fct_drop(formula))) %>%
  mutate(hyperparameters = str_c("terms = ", terms, ", bound = ", bound)) %>%
  mutate(hyperparameters = fct_rev(fct_inorder(hyperparameters))) %>%
  ggplot(aes(x = mean, xmin = mean - 2*se, xmax = mean + 2*se,
             y = hyperparameters)) +
  facet_wrap(facets = vars(formula)) +
  # geom_vline(xintercept = 0.5, linetype = "dashed") +
  geom_pointrange() +
  labs(x = "AUROC", y = NULL)
```

It appears that the number of terms is less consequential than the point bounds, though we compared models using a larger spread of the latter. (The bounds seem more important when predicting genotype either SZ or ZZ in contrast to others.)
We see AUROC improvements of between $0.02$ and $0.04$ from the strictest to the loosest parameter settings.
We will need to decide whether additional terms or larger scores are warranted for a roughly 3-point gain in AUROC---though the gains are smaller when using only medical history to predict genotype.

# Phase 3: Model evaluation

## Protocol

In this section, i lay out the protocol for the evaluation of candidates and selection of a final model specification.

### Selection of optimized model

The only additional predictor that improves both LR and FR models is smoking history, and the improvement appears to be substantial.
This could be carefully tested in the LR models via a likelihood ratio test of the null hypothesis that the model with smoking history outperforms the model without, or by using the Akaike Information Criterion to quantify the trade-off between predictive performance and model complexity.
The answer is less straightforward for the FR models.

Finally, whether a LR or FR model is selected for the final risk score, we must decide how sparse to require it. This amounts to choosing a complexity penalty for the LR model or to choosing a term limit and coefficient range for the FR model.

### Obtaining risk scores from optimized models

FasterRisk provides ready-made risk scores; no further processing needs to be done.
Risk scores can be calculated by adding small integer multiples of the categorical attributes and then mapping these scores to their associated risk estimates (probabilities of abnormal genotype).

The linear regression model will undergo the following steps, adapted from Sullivan &al (2004):

1. Reference values will be defined for all predictors (for medical history, these will be 0, i.e. no diagnosis). A baseline risk estimate will be established for cases that take all reference values. (This corresponds to the intercept in the logistic regression model.)
2. A maximum point value for any predictor will be decided. The tentative choice is 25. This will cap the value of the coefficients used in the calculation of the risk score. Larger values will allow more predictors to appear in the risk score and more granularity in its risk estimates; smaller values will make calculations easier.
3. All coefficients will be multiplied and rounded, so that the largest value is the maximum point value. This obtains the point values. Predictors whose coefficients round to zero will be excluded from the risk score.
4. Each score $s$ (the point total) will be mapped to a risk estimate $p$ (a probability) via the link function that defines the logistic regression model:
$$p = \frac{1}{1 + \exp(s)}$$

### Final evaluation

```{r read in final model evaluations}
here::here("data/aatd-3-eval-final.rds") %>%
  read_rds() %>%
  # remove duplicate COPD model
  distinct() ->
  eval_data
# if (! "geno_class" %in% names(eval_data$predictions[[1L]])) {
#   eval_data$predictions[[1L]] <-
#     rename(eval_data$predictions[[1L]], geno_class = "class")
# }
for (r in seq(nrow(eval_data))) {
  if (! "geno_class" %in% names(eval_data$predictions[[r]])) {
    eval_data$predictions[[r]] <-
      rename(eval_data$predictions[[r]], geno_class = "class")
  }
}
pop_class <- eval_data$predictions[[1L]] %>% count(geno_class) %>% deframe()
pop_risk <- pop_class[["Abnormal"]] / sum(pop_class)
```

We evaluate the selected model specifications (families and hyperparameter sets) by first fitting them to the entire $\frac{4}{5}$ training set from the second phase and then evaluating their predictions on the remaining $\frac{1}{5}$ testing set.
This provides our best estimate for the performances of the final candidate models on new data (of the same type as the data already collected). Remember that this is, by definition, data we do not currently have.
Our choice from among these models is based as much on interpretation as on performance.
The final model is then obtained by fitting the same family and hyperparameters to _the entire data set_. This final model cannot be evaluated using our data but provides our best proposal for a general risk score.

## Candidate models

In this section we compare several candidate models and select a final model to fit on the entire data set.

### Performance of candidate models

```{r ROC curves for candidate models}
eval_data %>%
  filter(model != "COPD") %>%
  rename(family = model) %>%
  mutate(specification = map_chr(
    hyperparameters,
    ~ str_c(names(.), unlist(.), sep = " = ", collapse = ", ")
  )) %>%
  unite(col = "model", family, specification, sep = ": ", remove = FALSE) %>%
  mutate(model = ifelse(
    str_detect(model, "FasterRisk"),
    str_c(model, (row_number() - 1L) %% 2L + 1L, sep = ", #"),
    model
  )) %>%
  mutate(model = fct_inorder(model)) %>%
  # OOPS
  mutate(predictions = map(
    predictions,
    ~ rename_with(., function(x) str_remove(x, "^geno_"))
  )) %>%
  unnest(hyperparameters) %>%
  unnest(predictions) %>%
  mutate(score = as.integer(score)) %>%
  group_by(predictors, response, family, abs_bound, model) %>%
  roc_curve(truth = class, estimate = score, event_level = "second") %>%
  ggplot(aes(x = specificity, y = sensitivity, group = model)) +
  facet_grid(cols = vars(predictors)) +
  coord_equal() +
  geom_path(aes(color = family, linetype = factor(abs_bound))) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  labs(color = "Model family", linetype = "Abs. bound") +
  theme(legend.position = "bottom")
```

Above are ROC curves for the 9 final candidate model specifications.
With a few exceptions, they are comparable in performance within each predictor set.
We can identify the lower-performing specifications in the area-under-the-curve comparison below.

```{r compare AUROC of final models}
eval_data %>%
  mutate(model = fct_inorder(model)) %>%
  unnest(metrics) %>%
  filter(.metric == "roc_auc") ->
  eval_auroc
eval_auroc %>%
  filter(model != "COPD") %>%
  mutate(specification = map_chr(
    hyperparameters,
    ~ str_c(names(.), unlist(.), sep = " = ", collapse = ", ")
  )) %>%
  ggplot(aes(x = .estimate, y = specification, color = model)) +
  facet_grid(rows = vars(predictors)) +
  geom_vline(
    data = select(distinct(filter(eval_auroc, model == "COPD")), -predictors),
    aes(xintercept = .estimate)
  ) +
  geom_point() +
  labs(x = "AUROC", y = NULL, color = NULL)
```

While the best-performing logistic regression models outperform the FasterRisk models, the logistic regression family also produces the worst-performing models when its regularization penalty is high and its point bound is small.
This may be because the penalty suppresses too many predictors; we will compare the predictor point values across all medical history--only models below.

### Score ranges of candidate models

The following two plots compare the resolutions and ranges of values taken by the candidate risk scores on the testing data.
A useful risk score will have sufficient resolution to distinguish cases that are different in consequential ways. However, its resolution will not be so high as to introduce large ranges of values that are never seen in practice.
A useful risk score will also effectively discriminate among cases in a population. Its value will range from low to high, and a more uniform distribution reflects greater discriminatory utility.
Note that the ordinate (vertical axis) is $\log(x+1)$-transformed. This is done to make small bars corresponding to rare risk score values more visible. The abscissa is always at 1-point resolution.
Because `FasterRisk` returns (at most) 2 models per specification, we represent each representation using "dodging" histograms in the same plotting window. (When the two returned models are the same, their numbering scheme is arbitrary and the histograms take different shades.)

```{r compare histograms of scores of candidate logistic regression models}
eval_data %>%
  filter(model == "logistic regression" & predictors == "Dx") %>%
  unnest(hyperparameters) %>%
  unnest(predictions) %>%
  ggplot(aes(x = score)) +
  facet_wrap(facets = vars(penalty, abs_bound), nrow = 2L, scales = "free_x",
             labeller = labeller(.cols = label_both, .multi_line = FALSE)) +
  geom_histogram(binwidth = 1L) +
  scale_y_continuous(trans = "log1p", breaks = 10 ^ seq(0L, 5L),
                     labels = scales::comma) +
  labs(x = "Score", y = NULL)
```

```{r compare histograms of scores of candidate FasterRisk models}
eval_data %>%
  filter(model == "FasterRisk" & predictors == "Dx") %>%
  mutate(number = as.character((row_number() - 1L) %% 2L + 1L)) %>%
  unnest(hyperparameters) %>%
  unnest(predictions) %>%
  ggplot(aes(x = score)) +
  facet_wrap(facets = vars(n_terms, abs_bound), nrow = 2L, scales = "free_x",
             labeller = labeller(.cols = label_both, .multi_line = FALSE)) +
  geom_histogram(aes(alpha = number), position = "dodge", binwidth = 1L) +
  scale_y_continuous(trans = "log1p", breaks = 10 ^ seq(0L, 5L),
                     labels = scales::comma) +
  scale_alpha_manual(values = c(.5, .9), guide = "none") +
  labs(x = "Score", y = NULL)
```

While the risk scores with greater coefficient bounds offer higher resolution, they do not improve overall discriminability.
The two higher-resolution risk scores detect the difference between the minimum score and the modal score---that is, the existence of a subpopulation of patients at lower-than-typical risk of abnormal genotype.
Higher resolutions also introduce considerable gaps in some cases, though they do introduce intermediate-risk values within the gaps present in the lower-resolution risk scores.
At least for the FasterRisk models, a point bound of 25 seems most useful; the benefit is less clear among the logistic regression models.

## Item point values in candidate models

```{r prepare point values}
eval_data %>%
  mutate(model = fct_inorder(model)) %>%
  # focus only on AUROC
  unnest(metrics) %>% filter(.metric == "roc_auc") %>%
  # distinguish FR models (necessary for pivoting below)
  mutate(number = case_when(
    model == "FasterRisk" ~ as.character((row_number() - 1L) %% 2L + 1L),
    TRUE ~ NA_character_
  )) %>%
  mutate(point_vals = map(
    point_vals,
    enframe, name = "item", value = "points"
  )) %>%
  mutate(AUROC = format(.estimate, digits = 3L, nsmall = 3L)) %>%
  mutate(specification = map_chr(
    hyperparameters,
    ~ str_c(names(.), unlist(.), sep = " = ", collapse = ", ")
  )) %>%
  mutate(specification = ifelse(
    model == "FasterRisk", str_c(specification, "; #", number), specification
  )) %>%
  unnest(hyperparameters, keep_empty = TRUE) %>%
  mutate(model_penal = case_when(
    model == "COPD" ~ "COPD",
    model == "logistic regression" ~
      str_c("LR", " (penalty = ", format(penalty, scientific = FALSE), ")"),
    model == "FasterRisk" ~
      str_c("FR", " #", number, " (terms ≤ ", n_terms, ")")
  )) %>%
  mutate(model_specs = case_when(
    model == "COPD" ~ "COPD",
    model == "logistic regression" ~
      str_c("LR (penalty = ", format(penalty, scientific = FALSE),
            "; bound = ", abs_bound, ")"),
    model == "FasterRisk" ~
      str_c("FR #", number, " (terms ≤ ", n_terms,
            "; bound = ", abs_bound, ")")
  )) ->
  eval_points
```

### Medical history only

```{r compare coefficients of candidate Dx models}
eval_points %>%
  filter(model != "COPD" & predictors == "Dx" & response == "ZZ") %>%
  select(-predictors, -response) %>%
  unnest(point_vals) %>%
  mutate(item = pred_names[item]) %>%
  mutate(points = as.integer(points)) ->
  eval_points_med
```

The table below compares the point values of the predictor items across all candidate models.
Each row is a candidate risk score.
We should first assess which would be most convenient in practice.
Then, we should assess whether any estimated improvement in performance makes others more valuable overall.
For ease of viewing, the items are sorted by greatest (ever) point value.
Areas under the ROC curve are reported in the rightmost column.

```{r Dx coefficient table}
eval_points_med %>%
  # sort by largest coefficients
  mutate(item = fct_rev(fct_reorder(item, points, function(x) max(abs(x))))) %>%
  pivot_wider(id_cols = c(model, specification, number, AUROC),
              names_from = item, values_from = points, names_sort = TRUE) %>%
  # remove items that are never used
  select(where(~ ! all(. == 0))) %>%
  select(-number) %>%
  # sort by largest coefficients
  arrange(across(c(-model, -specification, -AUROC), desc)) %>%
  relocate(AUROC, .after = everything()) %>%
  knitr::kable()
```

The relative magnitudes of the point values are visually compared in the plot below.

```{r Dx coefficient plot, fig.height=9}
eval_points_med %>%
  mutate(model_penal = fct_inorder(model_penal)) %>%
  mutate(abs_bound = fct_reorder(str_c("points ≤ ", abs_bound), abs_bound)) %>%
  # mutate(item = fct_reorder(item, points, function(x) max(abs(x)))) %>%
  mutate(item = factor(item, levels = unname(pred_names))) %>%
  mutate(sign = factor(sign(points), levels = c("-1", "1"))) %>%
  ggplot(aes(x = points, y = item, fill = sign)) +
  facet_grid(rows = vars(model_penal), cols = vars(abs_bound),
             scales = "free_x") +
  geom_vline(xintercept = 0) +
  geom_col() +
  scale_fill_discrete(aesthetics = c("color", "fill"), guide = "none") +
  labs(x = "Point value", y = NULL, fill = NULL)
```

On inspection, the following candidates stand out as exceptional:

1. The logistic regression model with low penalty and point value bound 25, which has an AUROC estimate of 0.618 using all 12 items. It approximately matches or exceeds the performance of the LR models with larger point value bounds.
2. The second FasterRisk model with a 10-item limit and point value bound 25, which approximately matches the performance of the aforementioned models at an AUROC of 0.617.
3. The first FasterRisk model with a 10-item limit and point value bound 100, which at AUROC 0.606 has similar performance to the other model obtained from the same batch. They both exclude self-reported COPD and family history of COPD, and this model excludes jaundice as an item whereas the other excludes bronchitis.

```{r select medical history candidates}
candidates_med <- c(
  `Candidate 1` = "LR (penalty = 0.0001; bound = 25)",
  `Candidate 2` = "FR #2 (terms ≤ 10; bound = 25)",
  `Candidate 3` = "FR #1 (terms ≤ 10; bound = 100)"
)
```

### Medical and smoking history

```{r compare coefficients of candidate Dx & smoking models}
eval_points %>%
  filter(model != "COPD" & predictors == "Dx+smoke use" & response == "ZZ") %>%
  select(-predictors, -response) %>%
  unnest(point_vals) %>%
  mutate(item = pred_names[item]) %>%
  mutate(points = as.integer(points)) ->
  eval_points_smoke
```

```{r Dx & smoking coefficient table}
eval_points_smoke %>%
  # sort by largest coefficients
  mutate(item = fct_rev(fct_reorder(item, points, function(x) max(abs(x))))) %>%
  pivot_wider(id_cols = c(model, specification, number, AUROC),
              names_from = item, values_from = points, names_sort = TRUE) %>%
  # remove items that are never used
  select(where(~ ! all(. == 0))) %>%
  select(-number) %>%
  # sort by largest coefficients
  arrange(across(c(-model, -specification, -AUROC), desc)) %>%
  relocate(AUROC, .after = everything()) %>%
  knitr::kable()
```

```{r Dx & smoking coefficient plot, fig.height=9}
eval_points_smoke %>%
  mutate(model_penal = fct_inorder(model_penal)) %>%
  mutate(abs_bound = fct_reorder(str_c("points ≤ ", abs_bound), abs_bound)) %>%
  # mutate(item = fct_reorder(item, points, function(x) max(abs(x)))) %>%
  mutate(item = factor(item, levels = unname(pred_names))) %>%
  mutate(sign = factor(sign(points), levels = c("-1", "1"))) %>%
  ggplot(aes(x = points, y = item, fill = sign)) +
  facet_grid(rows = vars(model_penal), cols = vars(abs_bound),
             scales = "free_x") +
  geom_vline(xintercept = 0) +
  geom_col() +
  scale_fill_discrete(aesthetics = c("color", "fill"), guide = "none") +
  labs(x = "Point value", y = NULL, fill = NULL)
```

Through discussion, we identified the following two models as of the greatest interest:

4. The logistic regression model with low penalty and point value bound 25, which achieves an estimated AUROC of 0.657. This nearly matches the best performance, by a more complicated LR model with a point value bound of 100. The model includes all 13 items.
5. The first FasterRisk model with a 10-item limit and point value bound of 100, which achieves an AUROC of 0.654, similar to that of several other FR models. It excludes both bronchitis and jaundice as well as self and family history of COPD.

```{r select medical and smoking history candidates}
candidates_smoke <- c(
  `Candidate 4` = "LR (penalty = 0.0001; bound = 25)",
  `Candidate 5` = "FR #1 (terms ≤ 10; bound = 100)"
)
```

## Risk scores and screening thresholds

```{r restrict to candidate scores}
eval_points %>%
  select(model, predictors, response,
         referent_risk, point_vals, score_risk, predictions, cutoff,
         AUROC, model_specs) %>%
  filter(
    model == "COPD" |
      (predictors == "Dx" & model_specs %in% candidates_med) |
      (predictors == "Dx+smoke use" & model_specs %in% candidates_smoke)
  ) %>%
  mutate(candidate = case_when(
    model == "COPD" ~ "Candidate 0",
    predictors == "Dx" ~
      names(candidates_med)[match(model_specs, candidates_med)],
    predictors == "Dx+smoke use" ~
      names(candidates_smoke)[match(model_specs, candidates_smoke)]
  )) %>%
  arrange(candidate) ->
  eval_cands
```

Here we recapitulate the five candidate models identified from the comparisons above:

```{r candidates coefficient table}
eval_cands %>%
  filter(model != "COPD") %>%
  select(-predictors, -response) %>%
  unnest(point_vals) %>%
  mutate(item = pred_names[item]) %>%
  mutate(points = as.integer(points)) %>%
  mutate(item = factor(item, levels = unname(pred_names))) %>%
  pivot_wider(id_cols = c(candidate, AUROC),
              names_from = item, values_from = points, names_sort = TRUE) %>%
  # sort by largest coefficients
  arrange(candidate) %>%
  relocate(AUROC, .after = everything()) %>%
  knitr::kable()
```

### Score--risk tables

The five models produce the following risk score tables.
Each table matches each value of one risk score to the estimated probability of a patient with that score having a ZZ genotype.

```{r examine risk scores, results='asis'}
revsum <- function(x) rev(cumsum(rev(x)))
for (r in seq(nrow(eval_cands))) {
  # compute for one slice at a time
  eval_cands %>%
    slice(r) ->
    eval_cands_slice
  # calculate percentage of the population screened at each threshold
  eval_cands_slice$predictions[[1L]] %>%
    count(score) %>%
    mutate(screen = revsum(n) / sum(n)) %>%
    select(-n) ->
    eval_screen
  # calculate total number of cases screened using each threshold
  eval_cands_slice$predictions[[1L]] %>%
    count(score, geno_class) %>%
    pivot_wider(names_from = geno_class, values_from = n, values_fill = 0L) %>%
    mutate(across(-score, revsum)) %>%
    mutate(across(-score, ~ format(., big.mark = ","))) ->
    eval_cases
  # pre-process and bind above calculations
  eval_cands_slice %>%
    unnest(score_risk) %>%
    mutate(score = ifelse(
      model == "FasterRisk",
      score - referent_risk, score
    )) %>%
    unnest(cutoff) %>%
    inner_join(eval_screen, by = "score") %>%
    inner_join(eval_cases, by = "score") %>%
    mutate(rel_risk = risk / pop_risk) %>%
    transmute(
      # Score = str_c(
      #   score, ifelse(risk >= .threshold & lag(risk < .threshold), "*", "")
      # ),
      Score = str_replace(format(score), "-", "\u2013"),
      Risk = str_c(
        format(round(risk, digits = 4L) * 100, nsmall = 1L, trim = TRUE), "%"
      ),
      RR = format(round(rel_risk, digits = 2L)),
      Screen = str_c(
        format(round(screen, digits = 4L) * 100, nsmall = 1L), "%"
      ),
      Normal, Abnormal
    ) %>%
    knitr::kable(
      align = "rrrrrr",
      caption = eval_cands$candidate[[r]],
      label = NA,
      format = "html", table.attr = "style='width:60%;'"
    ) %>%
    print()
}
```

### Final models

In this section we examine our single preferred model for each predictor set---medical history--only and medical-and-smoking-history.

```{r}
eval_cands %>%
  filter(candidate == "Candidate 0") %>%
  select(cutoff) %>% unnest(cutoff) %>%
  pull(specificity) ->
  copd_specificity
eval_cands %>%
  filter(candidate == "Candidate 0" |
           candidate == "Candidate 3" | candidate == "Candidate 5") %>%
  mutate(candidate = case_when(
    candidate == "Candidate 0" ~ "Guidelines",
    candidate == "Candidate 3" ~ "Medical only",
    candidate == "Candidate 5" ~ "Medical + Smoking"
  )) %>%
  mutate(candidate = fct_inorder(candidate)) %>%
  # pull(predictions) %>% {.[[1]]} %>% pull(geno_class) %>% levels()
  # OOPS
  mutate(predictions = map(
    predictions,
    mutate, geno_class = factor(geno_class, c("Normal", "Abnormal"))
  )) %>%
  # identify scores on either side of COPD specificity
  mutate(roc = map(
    predictions,
    roc_curve, truth = geno_class, score, event_level = "second"
  )) %>%
  unnest(roc) %>%
  mutate(spec_sign = sign(specificity - copd_specificity)) %>%
  group_by(model, predictors, response, spec_sign) %>%
  mutate(cutoff = case_when(
    model == "COPD" & .threshold == 1 ~ TRUE,
    model == "COPD" ~ FALSE,
    spec_sign == -1L ~ specificity == max(specificity),
    spec_sign == 1L ~ specificity == min(specificity),
    TRUE ~ TRUE
  )) %>%
  # select(.threshold, specificity, sensitivity, cutoff) %>% filter(cutoff)
  filter(cutoff) %>%
  ungroup() %>%
  # OOPS: correct scores
  mutate(score_risk = if_else(
    referent_risk %% 1 == 0,
    map2(score_risk, referent_risk, ~ transmute(.x, score = score - .y, risk)),
    map(score_risk, ~ select(., score, risk))
  )) %>%
  mutate(score_risk = map2(
    .threshold, score_risk,
    ~ filter(.y, score == .x)
  )) %>%
  # select(score_risk) %>% unnest(score_risk)
  unnest(score_risk) %>%
  rename_with(~ str_c("cutoff_", .), c(score, risk)) ->
  eval_cands_2side
```

### ROC curves

```{r compare overall performance, eval=FALSE}
eval_cands %>%
  mutate(predictions = map(
    predictions,
    ~ rename_with(., function(x) str_remove(x, "^geno_"))
  )) %>%
  unnest(predictions) %>%
  mutate(class = factor(class, c("Normal", "Abnormal"))) %>%
  group_by(candidate) %>%
  roc_curve(truth = class, estimate = score, event_level = "second") %>%
  ggplot(aes(x = specificity, y = sensitivity, group = candidate)) +
  coord_equal() +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_path(aes(color = candidate)) +
  scale_color_manual(values = c("black", RColorBrewer::brewer.pal(
    n = length(candidates_med) + length(candidates_smoke),
    name = "Dark2"
  ))) +
  labs(color = NULL)
```

```{r plot separate ROC curves with score labels, fig.height=8, eval=FALSE}
eval_cands %>%
  select(candidate, cutoff) %>%
  unnest(cutoff) %>%
  select(-.threshold) %>%
  mutate(cutoff = TRUE) ->
  cand_cutoffs
eval_cands %>%
  mutate(predictions = map(
    predictions,
    ~ rename_with(., function(x) str_remove(x, "^geno_"))
  )) %>%
  unnest(predictions) %>%
  mutate(class = factor(class, c("Normal", "Abnormal"))) %>%
  group_by(candidate) %>%
  roc_curve(truth = class, estimate = score, event_level = "second") %>%
  ungroup() %>%
  # identify cutoff values
  left_join(cand_cutoffs, by = c("candidate", "specificity", "sensitivity")) %>%
  mutate(cutoff = ifelse(is.na(cutoff), FALSE, cutoff)) %>%
  # prepare labels
  mutate(label = case_when(
    .threshold == -Inf ~ "", #"-∞",
    .threshold == Inf ~ "", #"+∞",
    TRUE ~ as.character(.threshold)
  )) %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_wrap(facets = vars(candidate)) +
  coord_equal() +
  geom_path() +
  geom_text(
    data = ~ filter(., ! cutoff),
    aes(label = label), hjust = "left", vjust = "bottom"
  ) +
  geom_label(
    data = ~ filter(., cutoff),
    aes(label = label, hjust = "left", vjust = "bottom")
  ) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  labs(color = NULL) ->
  eval_cands_roc_facet
print(eval_cands_roc_facet)
ggsave(
  here::here("fig/report-eval-cand-roc-facet.pdf"),
  eval_cands_roc_facet,
  width = textwidth, height = textwidth * 0.7
)
```

```{r compare overall performance of final models}
eval_cands_2side %>%
  mutate(predictions = map(
    predictions,
    ~ rename_with(., function(x) str_remove(x, "^geno_"))
  )) %>%
  unnest(predictions) %>%
  mutate(class = factor(class, c("Normal", "Abnormal"))) %>%
  group_by(candidate) %>%
  roc_curve(truth = class, estimate = score, event_level = "second") %>%
  ggplot(aes(x = specificity, y = sensitivity, group = candidate)) +
  coord_equal() +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  geom_path(aes(color = candidate)) +
  geom_point(aes(color = candidate)) +
  scale_color_manual(values = c("black", RColorBrewer::brewer.pal(
    n = length(eval_cands_2side$candidate) - 1L,
    name = "Dark2"
  ))) +
  labs(color = NULL)
```

```{r plot separate ROC curves for final models with score labels, fig.height=4}
eval_cands_2side %>%
  select(candidate, cutoff_score, specificity, sensitivity, cutoff) ->
  cand_cutoffs
eval_cands_2side %>%
  mutate(predictions = map(
    predictions,
    ~ rename_with(., function(x) str_remove(x, "^geno_"))
  )) %>%
  unnest(predictions) %>%
  mutate(class = factor(class, c("Normal", "Abnormal"))) %>%
  group_by(candidate) %>%
  roc_curve(truth = class, estimate = score, event_level = "second") %>%
  ungroup() %>%
  # identify cutoff values
  left_join(cand_cutoffs, by = c("candidate", "specificity", "sensitivity")) %>%
  # shift scores so that minimum total is zero for each
  group_by(candidate) %>% mutate(min_total = min(setdiff(.threshold, -Inf))) %>%
  mutate(across(c(.threshold, cutoff_score), ~ . - min_total)) %>%
  mutate(cutoff = ifelse(is.na(cutoff), FALSE, cutoff)) %>%
  # prepare labels
  mutate(label = case_when(
    .threshold == -Inf ~ "", #"-∞",
    .threshold == Inf ~ "", #"+∞",
    TRUE ~ as.character(.threshold)
  )) %>%
  ggplot(aes(x = specificity, y = sensitivity)) +
  facet_wrap(facets = vars(candidate)) +
  coord_equal() +
  geom_path() +
  geom_text(
    data = ~ filter(., ! cutoff),
    aes(label = label), hjust = "left", vjust = "bottom"
  ) +
  geom_label(
    data = ~ filter(., cutoff),
    aes(label = label, hjust = "left", vjust = "bottom")
  ) +
  geom_abline(intercept = 1, slope = -1, lty = 3) +
  labs(color = NULL) ->
  eval_cands_roc_facet
print(eval_cands_roc_facet)
ggsave(
  here::here("fig/report-eval-cand-roc-facet.pdf"),
  eval_cands_roc_facet,
  width = textwidth, height = textwidth * 0.4
)
```

### Screening thresholds

The following table compares the sensitivity and specificty of each risk score, used as a screening tool with the optimized threshold.
Recall that thresholds were optimized by selecting the specificity closest to that of our model of current screening guidelines.

```{r compare threshold performance}
eval_cands %>%
  unnest(cutoff) %>%
  # determine score at which risk cutoff is passed
  mutate(cutoff = map2_int(
    score_risk, .threshold,
    ~ .x$score[which(.x$risk >= .y & lag(.x$risk) < .y)]
  )) %>%
  mutate(cutoff = ifelse(
    model == "FasterRisk",
    cutoff - referent_risk, cutoff
  )) %>%
  transmute(
    Candidate = str_c(candidate, " (screen ≥ ", cutoff, ")"),
    `Risk` = .threshold,
    Sensitivity = sensitivity, Specificity = specificity
  ) %>%
  mutate(across(
    where(is.double),
    ~ str_c(format(round(., digits = 4L) * 100, digits = 4L), "%")
  )) %>%
  knitr::kable(format = "html", table.attr = "style='width:50%;'")
```

This table compares the sensitivity and specificity of only the current screening guidelines and the final risk scores, but considers thresholds with specificity on either side of the specificity of current guidelines.

```{r compare two-sided threshold performance}
eval_cands_2side %>%
  transmute(
    Candidate = str_c(candidate, " (screen ≥ ", cutoff_score, ")"),
    # `Score` = as.integer(score),
    `Risk` = cutoff_risk,
    Sensitivity = sensitivity, Specificity = specificity
  ) %>%
  mutate(across(
    # c(Sensitivity, Specificity),
    where(is.double),
    ~ str_c(format(round(., digits = 4L) * 100, digits = 4L), "%")
  )) %>%
  knitr::kable(format = "html", table.attr = "style='width:50%;'")
```

### Detection tables

```{r, results='asis', eval=FALSE}
eval_cands %>%
  unnest(cutoff) %>%
  unnest(predictions) %>%
  # OOPS
  mutate(class = ifelse(
    is.na(class),
    as.character(geno_class),
    as.character(class)
  )) %>%
  mutate(class = factor(class, c("Normal", "Abnormal"))) %>%
  group_by(candidate) %>%
  mutate(
    .pred_class = factor(
      ifelse(.pred_Abnormal >= .threshold, "Abnormal", "Normal"),
      levels = levels(class)
    )
  ) %>%
  count(candidate, class, .pred_class) %>%
  transmute(
    Candidate = candidate, Genotype = class,
    Amount = str_c(
      n, " (",
      format(round(n / sum(n), digits = 4L) * 100, digits = 2L),
      "%)"
    ),
    Screen = factor(ifelse(.pred_class == "Normal", "No", "Yes"),
                    levels = c("No", "Yes"))
  ) %>%
  ungroup() %>%
  pivot_wider(id_cols = c(Candidate, Screen),
              names_from = Genotype, values_from = Amount) %>%
  nest(table = -c(Candidate)) %>%
  pull(table) %>%
  `names<-`(eval_cands$candidate) %>%
  lapply(knitr::kable, align = "lrr",
         format = "html", table.attr = "style='width:30%;'")
```

```{r, results='asis', eval=FALSE}
eval_cands %>%
  unnest(cutoff) %>%
  unnest(predictions) %>%
  # OOPS
  mutate(class = ifelse(
    is.na(class),
    as.character(geno_class),
    as.character(class)
  )) %>%
  mutate(class = factor(class, c("Normal", "Abnormal"))) %>%
  group_by(candidate) %>%
  mutate(
    .pred_class = factor(
      ifelse(.pred_Abnormal >= .threshold, "Abnormal", "Normal"),
      levels = levels(class)
    )
  ) %>%
  count(candidate, class, .pred_class) %>%
  transmute(
    Candidate = candidate, Genotype = class,
    Amount = str_c(
      n, " (",
      format(round(n / sum(n), digits = 4L) * 100, digits = 2L),
      "%)"
    ),
    Screen = factor(ifelse(.pred_class == "Normal", "No", "Yes"),
                    levels = c("No", "Yes"))
  ) %>%
  ungroup() %>%
  pivot_wider(id_cols = c(Candidate, Screen),
              names_from = Genotype, values_from = Amount) %>%
  mutate(Candidate = ifelse(row_number() %% 2L == 1L, Candidate, "")) %>%
  knitr::kable(align = "llrr",
               format = "html", table.attr = "style='width:40%'")
```

The following tables summarize screening recommendations based on the two candidate models (plus COPD-based guidelines):

```{r, results='asis'}
eval_cands_2side %>%
  unnest(predictions) %>%
  mutate(geno_class = factor(geno_class, c("Normal", "Abnormal"))) %>%
  group_by(candidate) %>%
  # select(contains(".pred"), cutoff_risk, .threshold)
  mutate(
    .pred_class = factor(
      ifelse(.pred_Abnormal >= cutoff_risk, "Abnormal", "Normal"),
      levels = levels(geno_class)
    )
  ) %>%
  count(candidate, cutoff_score, geno_class, .pred_class) %>%
  group_by(candidate, cutoff_score) %>%
  transmute(
    Candidate = str_c(candidate, " (screen ≥ ", cutoff_score, ")"),
    Genotype = geno_class,
    Amount = str_c(
      n, " (",
      format(round(n / sum(n), digits = 4L) * 100, digits = 2L),
      "%)"
    ),
    Screen = factor(ifelse(.pred_class == "Normal", "No", "Yes"),
                    levels = c("No", "Yes"))
  ) %>%
  ungroup() %>%
  pivot_wider(id_cols = c(Candidate, Screen),
              names_from = Genotype, values_from = Amount) %>%
  nest(table = -c(Candidate)) %>%
  pull(table) %>%
  `names<-`(eval_cands_2side$candidate) %>%
  lapply(knitr::kable, align = "lrr",
         format = "html", table.attr = "style='width:30%;'")
```

```{r, results='asis'}
eval_cands_2side %>%
  unnest(predictions) %>%
  mutate(geno_class = factor(geno_class, c("Normal", "Abnormal"))) %>%
  group_by(candidate) %>%
  # select(contains(".pred"), cutoff_risk, .threshold)
  mutate(
    .pred_class = factor(
      ifelse(.pred_Abnormal >= cutoff_risk, "Abnormal", "Normal"),
      levels = levels(geno_class)
    )
  ) %>%
  count(candidate, cutoff_score, geno_class, .pred_class) %>%
  group_by(candidate, cutoff_score) %>%
  transmute(
    Candidate = str_c(candidate, " (screen ≥ ", cutoff_score, ")"),
    Genotype = geno_class,
    Amount = str_c(
      n, " (",
      format(round(n / sum(n), digits = 4L) * 100, digits = 2L),
      "%)"
    ),
    Screen = factor(ifelse(.pred_class == "Normal", "No", "Yes"),
                    levels = c("No", "Yes"))
  ) %>%
  ungroup() %>%
  pivot_wider(id_cols = c(Candidate, Screen),
              names_from = Genotype, values_from = Amount) %>%
  mutate(Candidate = ifelse(row_number() %% 2L == 1L, Candidate, "")) %>%
  knitr::kable(align = "llrr",
               format = "html", table.attr = "style='width:50%'")
```

### Additional ZZ cases recovered by proposed risk score versus COPD

```{r}
eval_cands %>%
  filter(candidate == "Candidate 0" | candidate == "Candidate 3") %>%
  select(candidate, cutoff, predictions) %>%
  unnest(cutoff) %>%
  unnest(predictions) %>%
  transmute(
    record_id, candidate,
    screen = ifelse(.pred_Abnormal >= .threshold, "Y", "N"),
    genotype = geno_class
  ) %>%
  filter(genotype == "Abnormal") %>%
  pivot_wider(names_from = candidate, values_from = screen) %>%
  select(
    record_id,
    Dx = `Candidate 0`, Dx_smoke = `Candidate 3`
  ) %>%
  filter(Dx != Dx_smoke) %>%
  arrange(Dx, Dx_smoke) %>%
  left_join(read_rds(here::here("data/aatd-pred.rds")), by = "record_id") %>%
  mutate(birth = year(date_of_birth), gender = str_sub(gender, 1L, 1L)) %>%
  select(
    birth, gender,
    any_of(names(pred_names)), smoking_use,
    Dx, Dx_smoke
  ) %>%
  select(-ends_with("_none")) %>%
  mutate(across(
    c(starts_with("lung_"), starts_with("liver_")),
    as.integer
  )) %>%
  rename_with(~ pred_names[.], any_of(names(pred_names))) %>%
  knitr::kable()
```

### Additional ZZ cases recovered by inclusion of smoking history

```{r}
eval_cands %>%
  filter(candidate == "Candidate 3" | candidate == "Candidate 5") %>%
  select(candidate, cutoff, predictions) %>%
  unnest(cutoff) %>%
  unnest(predictions) %>%
  transmute(
    record_id, candidate,
    screen = ifelse(.pred_Abnormal >= .threshold, "Y", "N"),
    genotype = geno_class
  ) %>%
  filter(genotype == "Abnormal") %>%
  pivot_wider(names_from = candidate, values_from = screen) %>%
  select(
    record_id,
    Dx = `Candidate 3`, Dx_smoke = `Candidate 5`
  ) %>%
  filter(Dx != Dx_smoke) %>%
  arrange(Dx, Dx_smoke) %>%
  left_join(read_rds(here::here("data/aatd-pred.rds")), by = "record_id") %>%
  mutate(birth = year(date_of_birth), gender = str_sub(gender, 1L, 1L)) %>%
  select(
    birth, gender,
    any_of(names(pred_names)), smoking_use,
    Dx, Dx_smoke
  ) %>%
  select(-ends_with("_none")) %>%
  mutate(across(
    c(starts_with("lung_"), starts_with("liver_")),
    as.integer
  )) %>%
  rename_with(~ pred_names[.], any_of(names(pred_names))) %>%
  knitr::kable()
```

### Comparison of COPD, Hx-only, and Hx+smoking screening for ZZ cases

```{r ggalluvial settings}
library(ggalluvial)
options(ggalluvial.reverse = FALSE,
        ggalluvial.lode.guidance = "zigzag")
```

The following alluvial plot tracks whether each ZZ case is flagged by each of three models: our model of conventional guidelines, and the two final risk scores with cutoffs closest in specificity to those guidelines.

```{r alluvial plot of nearest COPD-based cutoffs}
eval_cands %>%
  filter(candidate == "Candidate 0" |
           candidate == "Candidate 3" |
           candidate == "Candidate 5") %>%
  select(candidate, cutoff, predictions) %>%
  unnest(cutoff) %>%
  unnest(predictions) %>%
  transmute(
    record_id, candidate,
    screen = ifelse(.pred_Abnormal >= .threshold, "Y", "N"),
    genotype = geno_class
  ) %>%
  filter(genotype == "Abnormal") %>%
  pivot_wider(id_cols = record_id,
              names_from = candidate, values_from = screen) %>%
  unite(col = "Pattern", starts_with("Candidate "), remove = FALSE) %>%
  to_lodes_form(starts_with("Candidate "), key = "Model", value = "Screen") %>%
  mutate(Model = fct_recode(
    Model,
    COPD = "Candidate 0",
    Medical = "Candidate 3",
    `Medical+Smoking` = "Candidate 5"
  )) %>%
  mutate(Screen = fct_recode(Screen, Yes = "Y", No = "N")) %>%
  ggplot(aes(x = Model, stratum = Screen, alluvium = record_id)) +
  geom_alluvium(aes(fill = Pattern), curve_type = "cubic") +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = Screen), size = 4) +
  scale_fill_brewer(type = "qual") +
  labs(x = NULL) +
  guides(fill = "none") ->
  eval_cands_plot
print(eval_cands_plot)
ggsave(
  here::here("fig/report-eval-cand-alluvial-final.pdf"),
  eval_cands_plot,
  width = textwidth, height = textwidth * 0.6
)
```

The next alluvial plot tracks whether the ZZ cases across the current guidelines and two cutofss of the medical history-only risk score, just below and just above the COPD guidelines in specificity.

```{r alluvial plot of two-way COPD-based cutoffs for medical-only}
eval_cands %>%
  filter(candidate == "Candidate 0") %>%
  select(cutoff) %>% unnest(cutoff) %>%
  pull(specificity) ->
  copd_specificity
eval_cands %>%
  filter(candidate == "Candidate 0" | candidate == "Candidate 3") %>%
  # identify scores on either side of COPD specificity
  mutate(roc = map(
    predictions,
    roc_curve, truth = geno_class, score, event_level = "second"
  )) %>%
  unnest(roc) %>%
  mutate(spec_sign = sign(specificity - copd_specificity)) %>%
  group_by(model, predictors, response, spec_sign) %>%
  mutate(cutoff = case_when(
    model == "COPD" & .threshold == 1 ~ TRUE,
    model == "COPD" ~ FALSE,
    spec_sign == -1L ~ specificity == max(specificity),
    spec_sign == 1L ~ specificity == min(specificity),
    TRUE ~ TRUE
  )) %>%
  # select(.threshold, specificity, sensitivity, cutoff) %>% filter(cutoff)
  filter(cutoff) %>%
  ungroup() %>%
  # OOPS: correct scores
  mutate(score_risk = if_else(
    referent_risk %% 1 == 0,
    map2(score_risk, referent_risk, ~ transmute(.x, score = score - .y, risk)),
    map(score_risk, ~ select(., score, risk))
  )) %>%
  select(candidate, .threshold, predictions) %>%
  unnest(predictions) %>%
  filter(geno_class == "Abnormal") %>%
  transmute(
    record_id, candidate,
    model = ifelse(
      candidate == "Candidate 0", "Guidelines",
      str_c("Medical", " (", .threshold, "+)")
    ),
    screen = ifelse(score >= .threshold, "Y", "N"),
    genotype = geno_class
  ) %>%
  pivot_wider(id_cols = record_id,
              names_from = model, values_from = screen) %>%
  unite(col = "Pattern", -record_id, remove = FALSE) %>%
  to_lodes_form(
    c(Guidelines, starts_with("Medical")),
    key = "Model", value = "Screen"
  ) %>%
  mutate(Model = fct_relevel(
    Model,
    "Medical (0+)",
    "Guidelines",
    "Medical (1+)"
  )) %>%
  mutate(Screen = fct_recode(Screen, Yes = "Y", No = "N")) %>%
  ggplot(aes(x = Model, stratum = Screen, alluvium = record_id)) +
  geom_alluvium(aes(fill = Pattern), curve_type = "cubic") +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = Screen), size = 4) +
  scale_fill_brewer(type = "qual") +
  labs(x = NULL) +
  guides(fill = "none") ->
  eval_cands_plot
print(eval_cands_plot)
ggsave(
  here::here("fig/report-eval-cand-alluvial-twoside-medical.pdf"),
  eval_cands_plot,
  width = textwidth, height = textwidth * 0.6
)
```

Finally, this alluvial plot tracks whether the ZZ cases across the same three screening rules as above but also those obtained from the two final models with cutoffs on the opposite side of the specificity of current guidelines.

```{r alluvial plot of two-way COPD-based cutoffs}
eval_cands %>%
  filter(candidate == "Candidate 0") %>%
  select(cutoff) %>% unnest(cutoff) %>%
  pull(specificity) ->
  copd_specificity
eval_cands %>%
  filter(candidate == "Candidate 0" |
           candidate == "Candidate 3" | candidate == "Candidate 5") %>%
  # identify scores on either side of COPD specificity
  mutate(roc = map(
    predictions,
    roc_curve, truth = geno_class, score, event_level = "second"
  )) %>%
  unnest(roc) %>%
  mutate(spec_sign = sign(specificity - copd_specificity)) %>%
  group_by(model, predictors, response, spec_sign) %>%
  mutate(cutoff = case_when(
    model == "COPD" & .threshold == 1 ~ TRUE,
    model == "COPD" ~ FALSE,
    spec_sign == -1L ~ specificity == max(specificity),
    spec_sign == 1L ~ specificity == min(specificity),
    TRUE ~ TRUE
  )) %>%
  # select(.threshold, specificity, sensitivity, cutoff) %>% filter(cutoff)
  filter(cutoff) %>%
  ungroup() %>%
  # OOPS: correct scores
  mutate(score_risk = if_else(
    referent_risk %% 1 == 0,
    map2(score_risk, referent_risk, ~ transmute(.x, score = score - .y, risk)),
    map(score_risk, ~ select(., score, risk))
  )) %>%
  select(candidate, .threshold, predictions) %>%
  unnest(predictions) %>%
  filter(geno_class == "Abnormal") %>%
  transmute(
    record_id, candidate,
    model = case_when(
      candidate == "Candidate 0" ~ "Guidelines",
      candidate == "Candidate 3" ~ str_c("Medical", " (", .threshold, "+)"),
      candidate == "Candidate 5" ~
        str_c("Medical + Smoking", " (", .threshold, "+)")
    ),
    screen = ifelse(score >= .threshold, "Y", "N"),
    genotype = geno_class
  ) %>%
  pivot_wider(id_cols = record_id,
              names_from = model, values_from = screen) %>%
  unite(col = "Pattern", -record_id, remove = FALSE) %>%
  to_lodes_form(
    c(Guidelines, starts_with("Medical")),
    key = "Tool", value = "Screen"
  ) %>%
  mutate(Tool = fct_relevel(
    Tool,
    "Medical (0+)",
    "Medical + Smoking (-1+)",
    "Guidelines",
    "Medical + Smoking (0+)",
    "Medical (1+)"
  )) %>%
  mutate(Screen = fct_recode(Screen, Yes = "Y", No = "N")) %>%
  ggplot(aes(x = Tool, stratum = Screen, alluvium = record_id)) +
  geom_alluvium(aes(fill = Pattern), curve_type = "cubic") +
  geom_stratum() +
  geom_text(stat = "stratum", aes(label = Screen), size = 4) +
  # scale_fill_brewer(type = "qual") +
  labs(x = NULL) +
  coord_flip() +
  guides(fill = "none") ->
  eval_cands_plot
print(eval_cands_plot)
ggsave(
  here::here("fig/report-eval-cand-alluvial-twoside.pdf"),
  eval_cands_plot,
  width = textwidth, height = textwidth * 0.6
)
```

## Patient prototypes

We compute risk scores for a set of prototype patient histories and examine the relative contributions of the items that constitute the score.

```{r define hypothetical and challenge subjects}
which_of_n <- function(x, n) {
  res <- rep(0L, n)
  x <- x[x >= 0 & x <= n]
  res[x] <- 1L
  res
}
tibble(
  subject = c(seq(8), LETTERS[seq(4)], "X"),
  lung_hx_aat_deficiency_family = which_of_n(c(6,8), 13),
  liver_hx_cirrhosis = which_of_n(c(2,4), 13),
  lung_hx_chronic_bronchitis = which_of_n(c(3), 13),
  lung_hx_emphysema = which_of_n(c(1,2,5,9,11), 13),
  lung_hx_bronchiectasis = which_of_n(c(2), 13),
  liver_hx_hepatitis = which_of_n(c(), 13),
  liver_hx_abnormal_liver_function_test = which_of_n(c(4,12), 13),
  lung_hx_allergies = which_of_n(c(3,6,7,10), 13),
  liver_hx_jaundice = which_of_n(c(), 13),
  lung_hx_asthma = which_of_n(c(7), 13),
  lung_hx_copd = which_of_n(c(3,5,6), 13),
  lung_hx_family_history_copd = which_of_n(c(1,2,6,7,8,10,11), 13),
  smoking_use_Past = which_of_n(c(1,5,8,9,10), 13),
  smoking_use_Current = which_of_n(c(2,4,7,11), 13)
) %>%
  mutate(scenario = c(
    vector("list", 8L),
    list(
      "76-year-old male with OSA on BiPAP, morbid obesity, CAD s/p PCI, and AFib. He has a 32 pack-year history of tobacco use. PFTs show a moderate OVD without BDR. No personal history of liver disease. No family history of COPD. CT chest shows mild emphysematous changes in the apices.",
      "42-year-old female with DM1, allergies, and ESRD s/p renal transplant. Previous tobacco use, 7 pack-year history. Father with COPD. PFTs show a mild OVD and normal DLCO. Chest x-ray has mild hyperinflation.",
      "68-year-old female with depression/anxiety, hypertension, GERD, and chronic cough. Active tobacco use, 1/2 – 1 PPD x 42 years. PFTs show moderate OVD. CT chest shows mild emphysematous changes with tree-in-bud opacities. Sputum cultures recently grew MAC. History of father with COPD.",
      "47-year-old man with morbid obesity, hypertension, hyperlipidemia, GERD, DM2, NAFLD, ocassional alcohol use, and never smoker. AST/ALT mildly elevated. No jaundice and normal bilirubin. No family history of COPD. PFTs show RVD with normal DLCO."
    ),
    vector("list", 1L)
  )) ->
  subjects
```

```{r compute risk scores for challenge patients, results='asis'}
eval_cands %>%
  select(model, candidate, referent_risk, point_vals, score_risk, cutoff) %>%
  unnest(point_vals) %>%
  unnest(cutoff) %>% select(-sensitivity, -specificity) %>%
  mutate(points = as.integer(points)) %>%
  mutate(cutoff = map2_int(
    score_risk, .threshold,
    ~ pull(filter(.x, risk == .y), score)
  )) %>%
  mutate(cutoff = ifelse(
    model == "FasterRisk",
    cutoff - referent_risk, cutoff
  )) %>%
  select(candidate, item, points, cutoff) ->
  cand_points
subjects %>%
  select(-scenario) %>%
  pivot_longer(-subject, names_to = "item", values_to = "entry") %>%
  inner_join(cand_points, by = "item") %>%
  mutate(value = entry * points) %>%
  mutate(item = pred_names[item]) ->
  subjects_cand_points
subjects_cand_points %>%
  pivot_wider(c(subject, candidate),
              names_from = item, values_from = value) %>%
  rowwise() %>%
  mutate(total = sum(c_across(where(is.integer)), na.rm = TRUE)) %>%
  ungroup() %>%
  pivot_longer(-c(candidate, subject),
               names_to = "item", values_to = "value") %>%
  left_join(select(
    subjects_cand_points,
    candidate, subject, item, points, entry, value
  )) %>%
  mutate(tally = case_when(
    is.na(value) ~ "--",
    entry == 0L ~ "",
    # value == 0L ~ "",
    TRUE ~ format(value)
  )) %>%
  pivot_wider(c(candidate, subject),
              names_from = item, values_from = tally) %>%
  relocate(candidate, subject, any_of(unname(pred_names))) %>%
  left_join(distinct(subjects_cand_points, candidate, subject, cutoff)) %>%
  mutate(screen = ifelse(as.integer(total) >= cutoff, "Y", "N")) %>%
  nest(score = -c(candidate)) %>%
  {`names<-`(.$score, .$candidate)} %>%
  lapply(knitr::kable, align = c())
```



# Phase 4: Final model(s)

## Final model
